{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d3bdff101d217",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Modeling to assess tribal knowledge\n",
    "\n",
    "Our aim with this analysis is to create a methodology and a set of data-driven metrics that can help us identify potential technological risks within an organization's IT landscape. This proof of concept focuses on various risk factors that could affect operational health and security and influence technical debt hidden in our IT landscape. These metrics provide a quantitative basis for a risk diagnostic and enable us to see the impact of making directly relevant choices to reducing these risks. \n",
    "\n",
    "For our analysis, we focus on 7 major factors and rationale as to why:\n",
    "\n",
    "  - **Time Since Last Commit**: This metric indicates the recency of activity within a repository. Longer intervals since the last commit can suggest a project becomes dormant, highlighting potential maintenance or abandonment risks.\n",
    "  - **Age of the Repo**: his metric reflects the age of the repository from its initial creation to the present day. While a newer repository might be in active development or just gaining traction, an older repository with sporadic updates may indicate a legacy system that's either stable with minimal changes needed or potentially neglected.\n",
    "  - **Commit Frequency (CF)**: Regular commit activity shows ongoing development and maintenance. Low commit frequency can signal reduced development efforts or potential stagnation, implying operational risks.\n",
    "  - **Open Issues Ratio (OIR)**: The ratio of open to closed issues offers insights into how effectively a project addresses bugs, requests, or security concerns. A high open issues ratio may indicate operational inefficiencies or backlogs that could escalate into security or functional risks.\n",
    "  - **Pull Request Resolution Time (PRRT)**: The average time to close pull requests reflects the project's responsiveness and process efficiency. Longer resolution times may indicate bottlenecks in the development process, affecting the project's ability to respond promptly to emerging risks.\n",
    "  - **Total Number of Languages in the Repo**: Projects utilizing a wide range of programming languages might face increased complexity, potentially raising the risk of integration issues, maintenance challenges, and a higher barrier for new contributors, which could affect long-term sustainability.\n",
    "  - **Average Number of Commits per Month**: This metric provides an average rate of development activity over time, offering a more granulated view of the project's momentum. A declining trend may indicate a loss of development interest or a shift in focus, which could affect the project's vitality and risk profile.\n",
    "\n",
    "### Composite Function and Problem-Solving Capability\n",
    "\n",
    "When combined mathematically, the metrics above create a composite function that assesses risk across multiple dimensions. The model provides a comprehensive view of each repository's risk profile by combining these factors. This approach enables:\n",
    "\n",
    "- **Prioritization of Risk Mitigation Efforts**: Repositories or projects exhibiting higher risk signals across these metrics can be prioritized for further investigation and remediation efforts.\n",
    "- **Early Warning System:** Trends identified through these metrics can serve as early warnings for projects heading towards higher-risk states, allowing proactive interventions.\n",
    "- **Strategic Decision Support**: By understanding the risk landscape across the IT estate, decision-makers can make informed strategic choices about resource allocation, project continuation, and architectural adjustments.\n",
    "\n",
    "To understand the dynamics of these risks, we utilise Hierarchical Bayesian Modelling (HBM) as our analytical tool. HBM is adept at handling multi-level data, enabling us to dissect the layers of risk present in individual repositories and across the organisation as a whole.  For instance, a decline in commit activity and an increase in outdated dependencies may indicate a repository becoming less active or potentially being deprecated.  Conversely, a consistent or improving security posture alongside these trends might indicate a repository reaching a stage of maturity, requiring fewer updates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee99bdbedd31ef3",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "\n",
    "We extract key risk metrics through the GitHub REST API to construct a hierarchical Bayesian model for assessing risk. These metrics are crucial for creating and understanding compound metrics for our repositories' operational and security risks.\n",
    "\n",
    "**Data Collection and Preparation:** The initial step involves fetching and transforming data to construct these risk indicators across all repositories and owners via the GitHub API. This forms the base of our dataset, capturing a wide range of activities and potential vulnerabilities across the software development process.\n",
    "\n",
    "**Model Construction:** Using PyMC, we then define a hierarchical model that mirrors the layered nature of our data. This model distinguishes the variability within individual repositories and the typical patterns observable across different owners. By affecting repository-level distributions through owner-level priors, the model aims to uncover both specific and general risk trends.\n",
    "\n",
    "**Hierarchical Model Framework:**\n",
    "- **Level 1 (Observations):** Integrates base metrics as the observable layer.\n",
    "- **Level 2 (Risk Factors):** Introduces latent variables for different risk types (Operational health, Technical Debt) influenced by the Level 1 metrics.\n",
    "- **Level 3 (Overall Risk):** Our model aggregates the Level 2 risk factors into a compound risk score, reflecting the overall risk impact on the IT estate.\n",
    "\n",
    "**Inference and Analysis:** We sample from the posterior distribution by applying PyMC's MCMC methods to refine our model parameters. This allows us to investigate the posterior distributions and identify which languages or practices significantly deviate from the norm, marking them as potential risk areas.\n",
    "\n",
    "This approach enables a structured exploration of how various risk components interact and influence the overall risk landscape. PyMC's flexibility facilitates scenario simulation, such as evaluating the impact of improving pull request resolution time (PRRT) on the overall risk. This rigorous analysis gives us detailed insights into our IT estate's risk profile, supporting strategic decision-making and focused risk mitigation efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64929205d7d6c912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:33:12.085728Z",
     "start_time": "2024-03-20T10:32:54.788027Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import random \n",
    "import arviz as az\n",
    "import subprocess \n",
    "import pprint \n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tribal_knowledge.utils import LanguagePosteriorAnalysis, save_with_joblib, load_with_joblib\n",
    "from tribal_knowledge.repo_crawler import GitHubRepositoryInfoExtractor, PandasOutputFormat\n",
    "from github import Github, RateLimitExceededException\n",
    "from github.PaginatedList import PaginatedList\n",
    "from github.StatsCommitActivity import StatsCommitActivity\n",
    "from github.StatsParticipation import StatsParticipation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0862-2954-49f3-a206-e69b84cddea7",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "We have enabled two options to get data, \n",
    "  * Option 1: Using dummy data - This data is a copy of some data from the GitHub REST service and processed to have the same output as the data from the `RepoCrawler` class for the OWNER Apache.\n",
    "  * Option 2: We use the `RepoCrawler` class to pull data, and fields of interest are left to the user to define. This option requires a user to set a GitHub token.\n",
    "\n",
    "To use the second option, a personal access token must be given to the RepoCrawler (https://docs.github.com/en/enterprise-server@3.9/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c0712a-db93-4777-a50e-40d7a0dca1f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:33:12.095446Z",
     "start_time": "2024-03-20T10:33:12.088059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GITHUB_API_TOKEN retrieved\n"
     ]
    }
   ],
   "source": [
    "# Attempt to get the value of GITHUB_API_TOKEN environment variable - USER must set this either as global env var or you can set in a .env file\n",
    "load_dotenv('.env')\n",
    "github_api_token = os.environ.get('GITHUB_API_TOKEN')\n",
    "\n",
    "if github_api_token:\n",
    "    print(\"GITHUB_API_TOKEN retrieved\")\n",
    "else:\n",
    "    print(\"GITHUB_API_TOKEN is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0e1c0-0df6-48bf-be76-db9255f6b055",
   "metadata": {},
   "source": [
    "Depending on whether we receive a token, we will fetch data from a specified repository or load the dummy data. In this example, we will parse the Apache Foundation GitHub repository, which has around 3,000 repositories, and retrieve the programming language bytes and the number of stargazers for each repository.\n",
    "\n",
    "We've selected made use of the REST API in order to receive data for understanding the various dimensions of risk across repositories:\n",
    "\n",
    "To calculate *Commit Frequency* (CF) and *Average Number of Commits per Month*, we utilise the `get_stats_commit_activity` method. This method fetches data on commit activity over the past year, providing a basis for assessing repository maintenance and development momentum.\n",
    "\n",
    "*Time Since Last Commit* is inferred from the latest activity data obtained via `get_stats_participation`. While this method offers a broad stroke of engagement, pinpointing the exact date of the last commit may require calculation on the timestamps.\n",
    "\n",
    "The *Open Issues Ratio* (OIR) calculation relies on data from `get_issues` with the state set to \"open\". Completing this metric's calculation requires additional processing to contrast open against closed issues.\n",
    "\n",
    "*Pull Request Resolution Time* (PRRT) is determined by analysing data from `get_pulls` with the state as \"all\". This involves calculating the duration between the creation and closure of pull requests, which tells us the efficiency of the project's review and integration processes.\n",
    "\n",
    "The *Total Number of Languages* in the Repo is directly fetched using `get_languages`, offering insights into the project's complexity and the diversity of its technological stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec9012c-1a19-4124-878d-bb069d0395c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T10:33:12.108827Z",
     "start_time": "2024-03-20T10:33:12.098455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository details loaded from ./data/apache_repos.joblib.\n"
     ]
    }
   ],
   "source": [
    "# We have saved the output of getting the full set of repo details in ./data/apache_repos.json to eliminate the need to make the API calls\n",
    "# To update this list, change rerun = True and delete the existing file\n",
    "\n",
    "random.seed(42)\n",
    "rerun = False\n",
    "data_file = \"./data/apache_repos.joblib\"\n",
    "owners = ['apache']  # Specify the GitHub owners from whom to fetch repositories\n",
    "details_spec = {\n",
    "    \"properties\": [\"created_at\"], \n",
    "    \"methods\": { \n",
    "        \"get_stats_commit_activity\": {},\n",
    "        \"get_stats_participation\": {},\n",
    "        \"get_issues\": {\"state\": \"all\"},\n",
    "        \"get_pulls\": {\"state\": \"all\"},\n",
    "        \"get_languages\": {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if rerun is required or file doesn't exist\n",
    "if rerun or not os.path.exists(data_file):\n",
    "    if github_api_token:\n",
    "        try:\n",
    "            with GitHubRepositoryInfoExtractor(access_token=github_api_token, max_workers=1) as extractor:\n",
    "                repos = extractor.fetch_repositories(identifiers=owners)\n",
    "                # Fetch repository details\n",
    "                # Getting a random selection of 250 repositories\n",
    "                selected_repos = random.sample(repos, 250)\n",
    "                repo_attributes = extractor.fetch_repository_details(repos=selected_repos, details_spec=details_spec)\n",
    "                # Save to file using the save_json function\n",
    "                save_with_joblib(repo_attributes, data_file)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    else:\n",
    "        print(\"GITHUB_API_TOKEN is not set. Please set it to proceed.\")\n",
    "else:\n",
    "    # Load from a file if rerun is False and file exists\n",
    "    try:\n",
    "        repo_attributes = load_with_joblib(data_file)\n",
    "        print(f\"Repository details loaded from {data_file}.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f81a3-b1b8-4c52-8d09-48f22286f26e",
   "metadata": {},
   "source": [
    "GitHub's API methods through PyGithub are successful; the raw objects returned by these calls are directly stored in the results dictionary. This is typical behaviour when working with PyGithub, as the library returns specialised objects for API responses, which include paginated lists for iterable results (like issues or pull requests) and custom objects for structured data (like commit activities or language statistics).\n",
    "\n",
    "Let's unpack these to be a little more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ebf46b6-01e6-4063-8e9e-8533b6bc1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_paginated_items(paginated_list, condition=lambda x: True):\n",
    "    \"\"\"Count items in a PaginatedList that meet the given condition.\"\"\"\n",
    "    return sum(1 for item in paginated_list if condition(item))\n",
    "\n",
    "def process_repository_details(repo_name, repo_data):\n",
    "    processed_data = {}\n",
    "    \n",
    "    # Handle created_at date\n",
    "    if \"created_at\" in repo_data:\n",
    "        created_at = repo_data[\"created_at\"]\n",
    "        current_date = datetime.now(created_at.tzinfo)\n",
    "        age_days = (current_date - created_at).days\n",
    "        processed_data[\"age_in_days\"] = age_days\n",
    "\n",
    "    # Handle languages\n",
    "    if \"get_languages\" in repo_data:\n",
    "        languages = repo_data.get(\"get_languages\", {})\n",
    "        total_bytes = sum(languages.values())\n",
    "        total_languages = len(languages.keys())\n",
    "        processed_data[\"total_language_bytes\"] = total_bytes\n",
    "        processed_data[\"total_languages\"] = total_languages\n",
    "\n",
    "    # Handling issues\n",
    "    if \"get_issues\" in repo_data:\n",
    "        if isinstance(repo_data[\"get_issues\"], PaginatedList):\n",
    "            issues = repo_data[\"get_issues\"]\n",
    "            open_issues_count = count_paginated_items(issues, condition=lambda x: x.state == \"open\")\n",
    "            closed_issues_count = count_paginated_items(issues, condition=lambda x: x.state == \"closed\")\n",
    "            total_issues = open_issues_count + closed_issues_count\n",
    "            open_issues_ratio = open_issues_count / total_issues if total_issues > 0 else 0\n",
    "            processed_data[\"open_issues_ratio\"] = open_issues_ratio\n",
    "            processed_data[\"open_issues_count\"] = open_issues_count\n",
    "            processed_data[\"closed_issues_count\"] = closed_issues_count\n",
    "            # Deleting 'get_issues' object after processing for space\n",
    "            del repo_data[\"get_issues\"]\n",
    "\n",
    "    # Handling pulls \n",
    "    if \"get_pulls\" in repo_data:\n",
    "        if isinstance(repo_data[\"get_pulls\"], PaginatedList):\n",
    "            pulls = repo_data[\"get_pulls\"]\n",
    "            merged_pulls_count = sum(1 for pull in pulls if pull.merged_at is not None)\n",
    "            processed_data[\"merged_pulls_count\"] = merged_pulls_count\n",
    "            # Deleting 'get_pulls' after processing\n",
    "            del repo_data[\"get_pulls\"]\n",
    "\n",
    "    # Handling commit \n",
    "    if \"get_stats_commit_activity\" in repo_data:\n",
    "        if isinstance(repo_data[\"get_stats_commit_activity\"], list):\n",
    "            commit_activity = repo_data[\"get_stats_commit_activity\"]\n",
    "            total_commits_last_year = sum(activity.total for activity in commit_activity)\n",
    "            processed_data[\"total_commits_last_year\"] = total_commits_last_year\n",
    "            # Deleting 'get_stats_commit_activity' after processing\n",
    "            del repo_data[\"get_stats_commit_activity\"]\n",
    "            \n",
    "    # Handling participation\n",
    "    if \"get_stats_participation\" in repo_data:\n",
    "        if isinstance(repo_data[\"get_stats_participation\"], StatsParticipation):\n",
    "            participation = repo_data[\"get_stats_participation\"]\n",
    "            total_commits = sum(participation.all)\n",
    "            processed_data[\"total_commits_participation\"] = total_commits\n",
    "            # Deleting 'get_stats_participation' after processing\n",
    "            del repo_data[\"get_stats_participation\"]\n",
    "            \n",
    "\n",
    "    return repo_name, processed_data\n",
    "\n",
    "\n",
    "def process_all_repositories(repo_attributes, max_workers=1):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_repo = {executor.submit(process_repository_details, repo_name, repo_data): repo_name for repo_name, repo_data in repo_attributes.items()}\n",
    "        progress = tqdm(as_completed(future_to_repo), total=len(repo_attributes), desc=\"Processing Repositories\")\n",
    "\n",
    "        for future in progress:\n",
    "            repo_name = future_to_repo[future]\n",
    "            try:\n",
    "                _, processed_data = future.result()\n",
    "                repo_attributes[repo_name].update(processed_data)\n",
    "            except Exception as exc:\n",
    "                print(f'{repo_name} generated an exception: {exc}')\n",
    "                \n",
    "    return repo_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d422de-ffe9-4cb4-ab87-8baa3d445617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository details successfully read from ./data/apache_repo_attributes.joblib.\n"
     ]
    }
   ],
   "source": [
    "# Let us process things - again, we have built-in multi-threading for speed if required. However, watch the API rate limits of Github \n",
    "# We have saved a copy of this data in /data/processed_repo_attributes.json for convenience \n",
    "\n",
    "processed_data_file = './data/apache_repo_attributes.joblib'\n",
    "\n",
    "# Check if we need to rerun processing or load existing data\n",
    "if not os.path.exists(processed_data_file):\n",
    "    # Assuming `process_all_repositories` is a function that processes your data\n",
    "    repo_attributes = process_all_repositories(repo_attributes, max_workers=2)\n",
    "    \n",
    "    # Save processed data using the generic save function\n",
    "    save_with_joblib(repo_attributes, processed_data_file)\n",
    "else:\n",
    "    # Load the repository details from the file using the generic load function\n",
    "    repo_attributes = load_with_joblib(processed_data_file)\n",
    "    print(f\"Repository details successfully read from {processed_data_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a46d63d-71e6-4d76-8a68-539cd5578d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apache/cordova-plugin-statusbar',\n",
      "  {'age_in_days': 3669,\n",
      "   'closed_issues_count': 232,\n",
      "   'created_at': datetime.datetime(2014, 3, 15, 7, 0, 7, tzinfo=datetime.timezone.utc),\n",
      "   'get_languages': {'Java': 8815, 'JavaScript': 9709, 'Objective-C': 17298},\n",
      "   'merged_pulls_count': 82,\n",
      "   'open_issues_count': 29,\n",
      "   'open_issues_ratio': 0.1111111111111111,\n",
      "   'total_commits_last_year': 8,\n",
      "   'total_commits_participation': 8,\n",
      "   'total_language_bytes': 35822,\n",
      "   'total_languages': 3}),\n",
      " ('apache/ant-antlibs-s3',\n",
      "  {'age_in_days': 761,\n",
      "   'closed_issues_count': 0,\n",
      "   'created_at': datetime.datetime(2022, 3, 1, 4, 25, 20, tzinfo=datetime.timezone.utc),\n",
      "   'get_languages': {'Java': 258102},\n",
      "   'merged_pulls_count': 0,\n",
      "   'open_issues_count': 0,\n",
      "   'open_issues_ratio': 0,\n",
      "   'total_commits_last_year': 0,\n",
      "   'total_commits_participation': 0,\n",
      "   'total_language_bytes': 258102,\n",
      "   'total_languages': 1}),\n",
      " ('apache/incubator-seata-go',\n",
      "  {'age_in_days': 1451,\n",
      "   'closed_issues_count': 601,\n",
      "   'created_at': datetime.datetime(2020, 4, 10, 9, 20, 34, tzinfo=datetime.timezone.utc),\n",
      "   'get_languages': {'Go': 1202433, 'Makefile': 3310, 'Shell': 2797},\n",
      "   'merged_pulls_count': 278,\n",
      "   'open_issues_count': 51,\n",
      "   'open_issues_ratio': 0.07822085889570553,\n",
      "   'total_commits_last_year': 47,\n",
      "   'total_commits_participation': 47,\n",
      "   'total_language_bytes': 1208540,\n",
      "   'total_languages': 3}),\n",
      " ('apache/incubator-kie-kogito-online-staging',\n",
      "  {'age_in_days': 1096,\n",
      "   'closed_issues_count': 1,\n",
      "   'created_at': datetime.datetime(2021, 3, 31, 17, 57, tzinfo=datetime.timezone.utc),\n",
      "   'get_languages': {'CSS': 19161519,\n",
      "                     'HTML': 208076,\n",
      "                     'Java': 221660,\n",
      "                     'JavaScript': 5530,\n",
      "                     'Less': 17692599},\n",
      "   'merged_pulls_count': 1,\n",
      "   'open_issues_count': 0,\n",
      "   'open_issues_ratio': 0.0,\n",
      "   'total_commits_last_year': 40,\n",
      "   'total_commits_participation': 41,\n",
      "   'total_language_bytes': 37289384,\n",
      "   'total_languages': 5}),\n",
      " ('apache/hudi',\n",
      "  {'age_in_days': 2664,\n",
      "   'closed_issues_count': 10038,\n",
      "   'created_at': datetime.datetime(2016, 12, 14, 15, 53, 41, tzinfo=datetime.timezone.utc),\n",
      "   'get_languages': {'ANTLR': 264310,\n",
      "                     'Dockerfile': 37723,\n",
      "                     'Java': 20436965,\n",
      "                     'Mustache': 3826,\n",
      "                     'Python': 39028,\n",
      "                     'Scala': 5056584,\n",
      "                     'Shell': 195395,\n",
      "                     'Thrift': 3515},\n",
      "   'merged_pulls_count': 5861,\n",
      "   'open_issues_count': 902,\n",
      "   'open_issues_ratio': 0.08244972577696527,\n",
      "   'total_commits_last_year': 1195,\n",
      "   'total_commits_participation': 1207,\n",
      "   'total_language_bytes': 26037346,\n",
      "   'total_languages': 8})]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(repo_attributes.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fe316-0f6a-4b26-8623-8f30ecb6b171",
   "metadata": {},
   "source": [
    "## Hierarchical Model Specification\n",
    "\n",
    "This section delves into the Hierarchical Bayesian Modelling (HBM) framework, a robust statistical approach for analysing complex, multi-level datasets. We assess technological risks within an organisation's software repositories, which are critical to IT risk management and operational health maintenance.\n",
    "\n",
    "#### Introduction to Hierarchical Bayesian Modelling\n",
    "Hierarchical Bayesian Modelling is a statistical framework that facilitates data analysis across different levels of hierarchy by integrating the variability within individual units, such as repositories, and observing commonalities that may emerge. At the core of Bayesian inference is Bayes' theorem, which updates the probability for a hypothesis by introducing new evidence. A fundamental concept in HBM is exchangeability, suggesting that data points are probabilistically interchangeable. This characteristic makes it particularly suited for modelling datasets without a natural ordering but considered identically distributed given some parameters.\n",
    "\n",
    "Our analysis focuses on the characteristics of repositories. In the IT and software development context, we aim to understand and mitigate risks associated with software/products to maintain operational health and manage technical debt. To achieve this, we use Hierarchical Bayesian Modeling (HBM) to assess these risks quantitatively. We focus on several key metrics indicative of repository health and activity that are available to us.\n",
    "\n",
    "- **Time Since Last Commit (TSLC)**\n",
    "- **Age of the Repository (AR)**\n",
    "- **Commit Frequency (CF)**\n",
    "- **Open Issues Ratio (OIR)**\n",
    "- **Pull Request Resolution Time (PRRT)**\n",
    "- **Total Number of Languages in the Repo (TNLR)**\n",
    "- **Average Number of Commits per Month (ANCM)**\n",
    "\n",
    "For this POC, these metrics are readily available as observable variables (that we can pull from GitHub) that inform us about the potential risks associated with each repository within an organisation's IT landscape.\n",
    "\n",
    "## Hierarchical Bayesian Modelling Specification\n",
    "\n",
    "### **Level 1 (Observations)**\n",
    "\n",
    "At the base level, we model observable metrics from each repository, and are modelled using distributions chosen for their fit to the nature of the data:\n",
    "\n",
    "- **Time Since Last Commit (TSLC):**\n",
    "  $$ TSLC_i \\sim Exponential(\\lambda_{TSLC}) $$\n",
    "    The Exponential distribution is employed to model the TSLC, emphasising the memoryless property intrinsic to this distribution. This choice reflects the continuous, time-dependent nature of commits, where each interval between commits is independent of the previous. The rate parameter $\\lambda_{TSLC}$ quantifies the rate at which new commits are made, offering insights into repositories' vitality and maintenance frequency. A higher $\\lambda_{TSLC}$ suggests a more actively maintained repository, while a lower value indicates potential dormancy or reduced activity levels.\n",
    "\n",
    "- **Age of the Repository (AR):**\n",
    "  $$ AR_i \\sim Normal(\\mu_{AR}, \\sigma_{AR}^2) $$\n",
    "    The Normal distribution for modelling the age of repositories (AR) is based on the assumption that, within an organisational portfolio, repository ages tend to cluster around a common mean with specific variability. This distribution captures the central tendency and dispersion of repository ages, providing a symmetrical model around the mean $\\mu_{AR}$. The parameters $\\mu_{AR}$ and $\\sigma_{AR}^2$, representing the mean and variance, encapsulate the average lifespan of repositories and the spread of ages. This can indicate whether an organisation tends to maintain older, possibly legacy systems or if it predominantly works with newer repositories.\n",
    "\n",
    "- **Commit Frequency (CF):**\n",
    "  $$ CF_i \\sim NegativeBinomial(\\mu_{CF_i}, \\alpha_{CF}) $$\n",
    "\n",
    "    The Negative Binomial distribution is selected for its flexibility in modelling count data that exhibits variance exceeding the mean, a common characteristic of committed activities. The parameters $\\mu_{CF_i}$ and $\\alpha_{CF}$ represent the expected commit frequency for repository $i$ and the distribution's dispersion parameter, capturing the variability in commit activities across different repositories.\n",
    "\n",
    "- **Open Issues Ratio (OIR):**\n",
    "  $$ OIR_i \\sim Beta(\\alpha_{OIR}, \\beta_{OIR}) $$\n",
    "\n",
    "    The Beta distribution is adept at modelling ratios and proportions that inherently lie between 0 and 1, making it an ideal choice for the open issues ratio. Parameters $\\alpha_{OIR}$ and $\\beta_{OIR}$ shape the distribution, enabling the model to reflect the diverse states of issue resolution practices within the organisation.\n",
    "\n",
    "- **Average Number of Commits per Month (ANCM):**\n",
    "  $$ ANCM_i \\sim NegativeBinomial(\\mu_{ANCM_i}, \\alpha_{ANCM}) $$\n",
    "\n",
    "    Like CF, the Negative Binomial distribution is utilised for ANCM to accommodate the overdispersion commonly seen in commit data. The expected average number of commits per month $\\mu_{ANCM_i}$ and dispersion $\\alpha_{ANCM}$ articulate the spread and central tendency of development activity over time.\n",
    "\n",
    "- **Pull Request Resolution Time (PRRT):**\n",
    "  $$ PRRT_i \\sim LogNormal(\\mu_{PRRT}, \\sigma_{PRRT}^2) $$\n",
    "\n",
    "    A logNormal distribution is employed to model the resolution time for pull requests, acknowledging the skewed nature of this metric, where most data points are low but can extend to very high values. The mean $\\mu_{PRRT}$ and variance $\\sigma_{PRRT}^2$ on a logarithmic scale cater to accurately depicting the wide range of resolution times observed.\n",
    "\n",
    "- **Total Number of Languages in the Repo (TNLR):**\n",
    "  $$ TNLR_i \\sim Poisson(\\lambda_{TNLR}) $$\n",
    "\n",
    "The Poisson distribution is suitable for modelling the count of distinct programming languages used within a repository. It is a metric typically exhibiting the properties of count data with a mean equal to its variance. The rate parameter $\\lambda_{TNLR}$ enables the model to capture the frequency of language diversity across the organisation's repositories.\n",
    "\n",
    "\n",
    "### Level 2 (Repository-Specific Factors)\n",
    "\n",
    "At Level 2 of our model, we explore how specific features of repositories, such as their age, influence critical metrics we're interested in, like the frequency of commits. Think of it as trying to understand the personal habits of a repository, just like you might look at how a person's age affects their level of physical activity.\n",
    "\n",
    "- **Effect of Repository Age on Commit Frequency:**\n",
    "  For example, we're curious if older repositories are more or less active than newer ones. To do this, we use a formula:\n",
    "  $$ \\log(\\mu_{CF_i}) = \\beta_{0,CF} + \\beta_{AR,CF} \\cdot AR_i $$\n",
    "\n",
    "    In simpler terms, this means we're looking at how the age of a repository ($AR_i$) changes its commit activity ($\\mu_{CF_i}$), on average. The $\\log$ part helps us deal with the wide range of activity levels smoothly. $\\beta_{0,CF}$ is our starting point — think of it the base level of activity we'd expect from any repository, regardless of age. $\\beta_{AR,CF}$ shows us how much we need to adjust our expectations based on how old the repository is.\n",
    "\n",
    "- **Pull Request Resolution Time (PRRT) and Technical Debt:**\n",
    "  The time to resolve pull requests (PRRT) can be a proxy for understanding technical debt within a repository. Longer PRRT might indicate complexities or inefficiencies within the codebase or workflow. We model PRRT using a log-normal distribution to accurately reflect the right-skewed nature of resolution times:\n",
    "  $$ PRRT_i \\sim LogNormal(\\mu_{PRRT}, \\sigma_{PRRT}^2) $$\n",
    "  Here, $\\mu_{PRRT}$ is modelled as the logarithm of the average resolution time, capturing the central tendency of PRRT on a log scale. $\\sigma_{PRRT}^2$ represents the variance, accounting for the spread in PRRT across repositories. This modeling choice allows us to quantify and potentially identify areas of high technical debt that could benefit from optimization.\n",
    "\n",
    "- **Open Issues Ratio (OIR) Adjustments for Repository Characteristics:**\n",
    "  The open issues ratio (OIR) gives insights into a repository's responsiveness and potential backlog issues. Adjusting OIR for specific repository characteristics can highlight areas for process improvements. We model OIR with a Beta distribution due to its bounded nature (0 to 1):\n",
    "  $$ OIR_i \\sim Beta(\\alpha_{OIR}, \\beta_{OIR}) $$\n",
    "  $\\alpha_{OIR}$ and $\\beta_{OIR}$ are shape parameters influenced by repository characteristics, guiding the distribution's form. This approach allows us to understand the variability in issue management practices and to target interventions to improve issue resolution efficiency.\n",
    "\n",
    "\n",
    "### Level 3 (Organisational Trends)\n",
    "\n",
    "At Level 3, we're zooming out to look at the entire organisation. We use what we've learned about individual repositories to identify broader patterns and trends. This is where we talk about \"hyperpriors,\" which set expectations for how these factors behave across all repositories.\n",
    "\n",
    "- **Variance of the Effect of Repository Age on Commit Frequency:**\n",
    "  $$ \\beta_{AR,CF} \\sim Normal(0, \\sigma_{AR,CF}^2) $$\n",
    "  $\\sigma_{AR,CF}^2$ represents the variance in the effect of repository age on commit frequency across the organisation.\n",
    "\n",
    "We're asking: \"Knowing how repository age affects commit activity, can we see a consistent pattern across the whole organisation, or is there a lot of variation?\" By exploring this, we can understand if there's a common ageing effect on repositories across the organisation or if each repository is unique in how it matures.\n",
    "\n",
    "To put all of this in plain terms, at Level 2, we're like detectives examining each repository's story — how its unique features contribute to its risk profile. Then, at Level 3, we step back to see if a common story is being told across the entire organisation, which can help strategise and manage risks more effectively globally. This hierarchical approach gives us a comprehensive view of risk across the organisation's IT landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666c696-acdd-48d5-ba48-dfd4480b6f0d",
   "metadata": {},
   "source": [
    "## Transitioning to Posterior Distributions\n",
    "\n",
    "### From Theory to Practice: The Role of Posterior Distributions\n",
    "\n",
    "With our model parameters defined and their priors set, the next step in Bayesian analysis is to update these beliefs with observed data. This is where the posterior distribution comes into play.\n",
    "\n",
    "#### What is the posterior?\n",
    "\n",
    "In the hierarchical Bayesian modelling (HBM) context, the posterior distribution is the updated belief about our model's parameters after considering the observed data. It combines our prior beliefs (the priors) and the evidence from the data (the likelihood). Mathematically, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(\\theta | data) \\propto P(data | \\theta) \\times P(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ P(\\theta | data) $\\) is the posterior distribution of the parameters \\($ \\theta $\\).\n",
    "- \\($ P(data | \\theta) $\\) is the likelihood of the data given the parameters.\n",
    "- \\($ P(\\theta) $\\) is the prior distribution of the parameters.\n",
    "\n",
    "After observing the data, the posterior distribution provides a range of likely values for the parameters, which is crucial for making informed decisions.\n",
    "\n",
    "\n",
    "### Leveraging Posterior Analysis for Strategic Insights\n",
    "\n",
    "Going from model specification to practical application culminates in the posterior analysis. This phase brings our hierarchical Bayesian model's theoretical foundations into actionable insights, informing strategic decisions and operational improvements, for example, we can analyze our posterior for \n",
    "\n",
    "- **Credible Intervals:** The crux of informed decision-making, these intervals offer a statistical snapshot of parameter certainty. A narrow credible interval around a model parameter, like commit frequency, indicates a high degree of confidence in our estimates, guiding where to focus efforts for improvement or investigation.\n",
    "\n",
    "- **Identifying Outliers:** Anomalies within the posterior distributions act as beacons, highlighting repositories or practices deviating from the norm. These outliers could signify areas of innovation deserving recognition and replication or, conversely, red flags indicating risks that warrant immediate attention.\n",
    "\n",
    "- **Optimising Resource Allocation:** With refined insights, organisations can strategically channel resources towards initiatives that promise the most significant impact. This could mean investing in areas showing potential based on posterior trends, bolstering practices around identified silos to prevent knowledge stagnation, or redirecting efforts from less productive or riskier ventures.\n",
    " \n",
    "Interpreting the posterior distributions derived from our hierarchical model enhances our understanding of our IT estate; it equips us to forecast and plan to be both efficient and resilient to future challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280421-54eb-4c68-8cb1-f28ebb96f6cb",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- Implementing the HBM using PyMC\n",
    "- Defining the model in PyMC\n",
    "- Setting up the priors for each level of the hierarchy\n",
    "- Incorporating the data into the model\n",
    "- Model fitting (e.g., using MCMC methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4281107a-2e4b-494b-aaeb-f633cd709d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Owner                           Repository                created_at  \\\n",
      "0  apache             cordova-plugin-statusbar 2014-03-15 07:00:07+00:00   \n",
      "1  apache                       ant-antlibs-s3 2022-03-01 04:25:20+00:00   \n",
      "2  apache                   incubator-seata-go 2020-04-10 09:20:34+00:00   \n",
      "3  apache  incubator-kie-kogito-online-staging 2021-03-31 17:57:00+00:00   \n",
      "4  apache                                 hudi 2016-12-14 15:53:41+00:00   \n",
      "\n",
      "   age_in_days  total_language_bytes  total_languages  open_issues_ratio  \\\n",
      "0         3669                 35822                3           0.111111   \n",
      "1          761                258102                1           0.000000   \n",
      "2         1451               1208540                3           0.078221   \n",
      "3         1096              37289384                5           0.000000   \n",
      "4         2664              26037346                8           0.082450   \n",
      "\n",
      "   open_issues_count  closed_issues_count  merged_pulls_count  \\\n",
      "0                 29                  232                  82   \n",
      "1                  0                    0                   0   \n",
      "2                 51                  601                 278   \n",
      "3                  0                    1                   1   \n",
      "4                902                10038                5861   \n",
      "\n",
      "   total_commits_last_year  total_commits_participation  \n",
      "0                      8.0                            8  \n",
      "1                      0.0                            0  \n",
      "2                     47.0                           47  \n",
      "3                     40.0                           41  \n",
      "4                   1195.0                         1207  \n"
     ]
    }
   ],
   "source": [
    "# Conver the json to a pandas dataframe for simpler indexing and utilisation\n",
    "\n",
    "pandas_output = PandasOutputFormat(nested_row=False)\n",
    "df = pandas_output.transform(repo_attributes)\n",
    "df = df.drop('get_stats_commit_activity', axis=1)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b882230e-57d8-4bf4-8cdd-4ec72954db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc as pm\n",
    "\n",
    "# Simulated data for demonstration\n",
    "n_repositories = 100\n",
    "repository_idx = np.random.randint(0, n_repositories, size=1000)\n",
    "\n",
    "# Observed data\n",
    "tslc = np.random.exponential(scale=1.0, size=1000)\n",
    "ar = np.random.normal(loc=5.0, scale=2.0, size=1000)\n",
    "cf = np.random.negative_binomial(n=10, p=0.5, size=1000)\n",
    "oir = np.random.beta(a=2.0, b=5.0, size=1000)\n",
    "ancm = np.random.negative_binomial(n=10, p=0.5, size=1000)\n",
    "prrt = np.random.lognormal(mean=2.0, sigma=0.5, size=1000)\n",
    "tnlr = np.random.poisson(lam=5, size=1000)\n",
    "\n",
    "\n",
    "# Define rhi_expression outside pm.Deterministic\n",
    "rhi_expression = (\n",
    "    weight_cf_prior * (cf_obs - np.log1p(cf_obs)) -  # Assuming cf_obs is correctly defined\n",
    "    weight_prrt_prior * np.log1p(prrt_obs) -\n",
    "    weight_oir_prior * oir_obs +\n",
    "    weight_tnlr_prior * np.sqrt(tnlr_obs) -  # Ensure this operation is valid\n",
    "    np.log1p(tslc_obs) +\n",
    "    weight_cf_prior * np.log1p(ancm_obs) -\n",
    "    weight_cf_prior * (cf_obs - cf_expected) +  # Ensure cf_expected is defined\n",
    "    weight_prrt_prior * (prrt_obs - prrt_expected)  # Ensure prrt_expected is defined\n",
    ")\n",
    "\n",
    "with pm.Model() as tribal_knowledge:\n",
    "\n",
    "  # Level 1: Direct Observations (Corrected)\n",
    "  # Directly observed metrics are modeled with their appropriate distributions.\n",
    "  tslc_obs = pm.Exponential('time_since_last_commit_obs', 1.0, observed=tslc)\n",
    "  ar_obs = pm.Normal('repository_age_obs', mu=5.0, sigma=2.0, observed=ar)\n",
    "  cf_obs = pm.NegativeBinomial('commit_frequency_obs', mu=10, alpha=0.5, observed=cf)\n",
    "  oir_obs = pm.Beta('open_issues_ratio_obs', alpha=2.0, beta=5.0, observed=oir)\n",
    "  ancm_obs = pm.NegativeBinomial('average_commits_per_month_obs', mu=10, alpha=0.5, observed=ancm)\n",
    "  prrt_obs = pm.LogNormal('pull_request_resolution_time_obs', mu=np.log(2.0), sigma=0.5, observed=prrt)\n",
    "  tnlr_obs = pm.Poisson('total_languages_obs', mu=5, observed=tnlr)\n",
    "\n",
    "  # Level 2: Repository-Specific Factors\n",
    "  # Modeling the influence of repository-specific characteristics on the observed metrics.\n",
    "  # Examples: Effect of Repository Age on Commit Frequency and Pull Request Resolution Time\n",
    "  # Define sigma_ar (hyperprior for variability)\n",
    "  sigma_ar = pm.HalfNormal('sigma_ar', sigma=1)\n",
    "\n",
    "  # Calculate conditioned alpha and beta (exponential relationship)\n",
    "  alpha_ar_prrt_conditioned = pm.Deterministic('alpha_ar_prrt_conditioned', np.exp(sigma_ar))\n",
    "  beta_ar_prrt_conditioned = pm.Deterministic('beta_ar_prrt_conditioned', np.exp(sigma_ar))\n",
    "\n",
    "  # Define beta_ar_prrt using conditioned values\n",
    "  beta_ar_prrt = pm.Beta('beta_ar_prrt', alpha=alpha_ar_prrt_conditioned, beta=beta_ar_prrt_conditioned)\n",
    "\n",
    "  # prrt_expected now uses the conditioned beta_ar_prrt\n",
    "  prrt_expected = pm.Deterministic('expected_pull_request_resolution_time', beta_0_prrt + beta_ar_prrt * ar_obs)\n",
    "\n",
    "  # Level 3: Organizational Trends\n",
    "  # Hyperpriors to capture variability across the organization.\n",
    "\n",
    "  # Hyperpriors for weights in the RHI calculation:\n",
    "  # Introduce hyperpriors for the weights used in the RHI calculation. Here, we use\n",
    "  # HalfNormal distributions to ensure positive weights. You can explore alternative\n",
    "  # distributions as needed.\n",
    "  weight_cf_sigma = pm.HalfNormal('weight_cf_sigma', sigma=1)\n",
    "  weight_cf_prior = pm.HalfNormal('weight_cf_prior', sigma=weight_cf_sigma)\n",
    "  weight_prrt_sigma = pm.HalfNormal('weight_prrt_sigma', sigma=1)\n",
    "  weight_prrt_prior = pm.HalfNormal('weight_prrt_prior', sigma=weight_prrt_sigma)\n",
    "  weight_oir_sigma = pm.HalfNormal('weight_oir_sigma', sigma=1)\n",
    "  weight_oir_prior = pm.HalfNormal('weight_oir_prior', sigma=weight_oir_sigma)  # Higher weight for open issues\n",
    "  weight_tnlr_sigma = pm.HalfNormal('weight_tnlr_sigma', sigma=1)  # Hyperprior for tnlr weight\n",
    "  weight_tnlr_prior = pm.HalfNormal('weight_tnlr_prior', sigma=weight_tnlr_sigma)  # Initial weight for tnlr\n",
    "\n",
    "  # RHI Calculation: Comprehensive Repository Health Index\n",
    "  rhi_composite = pm.Deterministic('repository_health_index', rhi_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb90ed6-45c6-4168-b09a-62dc09530702",
   "metadata": {},
   "source": [
    "Here’s the graphical representation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02f96ae5-f91b-4f08-b491-38a1e50d70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"2730pt\" height=\"593pt\"\n",
       " viewBox=\"0.00 0.00 2730.14 592.66\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 588.66)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-588.66 2726.14,-588.66 2726.14,4 -4,4\"/>\n",
       "<text text-anchor=\"middle\" x=\"1361.07\" y=\"-33\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Hierarchical tribal_knowledge Model</text>\n",
       "<text text-anchor=\"middle\" x=\"1361.07\" y=\"-9\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Bayesian Model Visualization</text>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster1000</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M20,-64C20,-64 1820,-64 1820,-64 1826,-64 1832,-70 1832,-76 1832,-76 1832,-375.27 1832,-375.27 1832,-381.27 1826,-387.27 1820,-387.27 1820,-387.27 20,-387.27 20,-387.27 14,-387.27 8,-381.27 8,-375.27 8,-375.27 8,-76 8,-76 8,-70 14,-64 20,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"1810.5\" y=\"-70.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1000</text>\n",
       "</g>\n",
       "<!-- repository_health_index -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>repository_health_index</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1552.12,-151 1401.88,-151 1401.88,-95.75 1552.12,-95.75 1552.12,-151\"/>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-133.7\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">repository_health_index</text>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-117.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-102.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- total_languages_obs -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>total_languages_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"1732\" cy=\"-226.07\" rx=\"91.92\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"1732\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">total_languages_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"1732\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1732\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Poisson</text>\n",
       "</g>\n",
       "<!-- total_languages_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>total_languages_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1665.33,-198.74C1631.9,-185.54 1591.18,-169.46 1556.17,-155.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1557.49,-152.4 1546.9,-151.98 1554.92,-158.91 1557.49,-152.4\"/>\n",
       "</g>\n",
       "<!-- pull_request_resolution_time_obs -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>pull_request_resolution_time_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"1477\" cy=\"-226.07\" rx=\"145.49\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pull_request_resolution_time_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1477\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">LogNormal</text>\n",
       "</g>\n",
       "<!-- pull_request_resolution_time_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>pull_request_resolution_time_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1477,-186.71C1477,-178.89 1477,-170.65 1477,-162.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1480.5,-162.88 1477,-152.88 1473.5,-162.88 1480.5,-162.88\"/>\n",
       "</g>\n",
       "<!-- open_issues_ratio_obs -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>open_issues_ratio_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"1210\" cy=\"-226.07\" rx=\"103.06\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"1210\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">open_issues_ratio_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"1210\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1210\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Beta</text>\n",
       "</g>\n",
       "<!-- open_issues_ratio_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>open_issues_ratio_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1282.25,-197.82C1316.97,-184.73 1358.75,-168.97 1394.69,-155.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1395.61,-158.81 1403.74,-152.01 1393.14,-152.26 1395.61,-158.81\"/>\n",
       "</g>\n",
       "<!-- commit_frequency_obs -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>commit_frequency_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"983\" cy=\"-226.07\" rx=\"105.71\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"983\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">commit_frequency_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"983\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"983\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">NegBinomial</text>\n",
       "</g>\n",
       "<!-- commit_frequency_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>commit_frequency_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1058.29,-198.29C1071.42,-194.15 1085.03,-190.2 1098,-187 1197.06,-162.58 1313.06,-144.98 1390.39,-134.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.55,-138.28 1400.01,-133.52 1389.64,-131.34 1390.55,-138.28\"/>\n",
       "</g>\n",
       "<!-- repository_age_obs -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>repository_age_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"673\" cy=\"-340.2\" rx=\"90.33\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"673\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">repository_age_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"673\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"673\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normal</text>\n",
       "</g>\n",
       "<!-- repository_age_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>repository_age_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M640.38,-303.43C613.61,-269.71 584.67,-219.61 615,-187 667.02,-131.06 1181.78,-124.49 1390.13,-124.14\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.1,-127.64 1400.1,-124.13 1390.1,-120.64 1390.1,-127.64\"/>\n",
       "</g>\n",
       "<!-- expected_pull_request_resolution_time -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>expected_pull_request_resolution_time</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"859.5,-253.69 624.5,-253.69 624.5,-198.44 859.5,-198.44 859.5,-253.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"742\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">expected_pull_request_resolution_time</text>\n",
       "<text text-anchor=\"middle\" x=\"742\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"742\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- repository_age_obs&#45;&gt;expected_pull_request_resolution_time -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>repository_age_obs&#45;&gt;expected_pull_request_resolution_time</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M695.78,-302.17C703.4,-289.79 711.89,-276 719.49,-263.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"722.31,-265.75 724.56,-255.4 716.34,-262.08 722.31,-265.75\"/>\n",
       "</g>\n",
       "<!-- time_since_last_commit_obs -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>time_since_last_commit_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"459\" cy=\"-226.07\" rx=\"126.93\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">time_since_last_commit_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"459\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Exponential</text>\n",
       "</g>\n",
       "<!-- time_since_last_commit_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>time_since_last_commit_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M548.69,-198.09C565.61,-193.79 583.25,-189.83 600,-187 886.8,-138.51 1232.54,-127.51 1390.58,-125.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.17,-128.56 1400.12,-124.91 1390.07,-121.56 1390.17,-128.56\"/>\n",
       "</g>\n",
       "<!-- average_commits_per_month_obs -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>average_commits_per_month_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"165\" cy=\"-226.07\" rx=\"148.67\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">average_commits_per_month_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"165\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">NegBinomial</text>\n",
       "</g>\n",
       "<!-- average_commits_per_month_obs&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>average_commits_per_month_obs&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.74,-197.46C286.03,-193.36 304.99,-189.64 323,-187 718.43,-129.13 1197.12,-123.4 1390.13,-123.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.08,-127.21 1400.08,-123.73 1390.09,-120.21 1390.08,-127.21\"/>\n",
       "</g>\n",
       "<!-- expected_pull_request_resolution_time&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>expected_pull_request_resolution_time&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M823.44,-197.99C838.15,-193.81 853.44,-189.92 868,-187 1051.99,-150.1 1271.86,-134.31 1390.61,-128.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1390.46,-131.58 1400.27,-127.57 1390.1,-124.59 1390.46,-131.58\"/>\n",
       "</g>\n",
       "<!-- alpha_ar_prrt_conditioned -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>alpha_ar_prrt_conditioned</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1783.25,-470.52 1618.75,-470.52 1618.75,-415.27 1783.25,-415.27 1783.25,-470.52\"/>\n",
       "<text text-anchor=\"middle\" x=\"1701\" y=\"-453.22\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">alpha_ar_prrt_conditioned</text>\n",
       "<text text-anchor=\"middle\" x=\"1701\" y=\"-437.47\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1701\" y=\"-421.72\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- beta_ar_prrt -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>beta_ar_prrt</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1900\" cy=\"-340.2\" rx=\"60.1\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"1900\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">beta_ar_prrt</text>\n",
       "<text text-anchor=\"middle\" x=\"1900\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1900\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Beta</text>\n",
       "</g>\n",
       "<!-- alpha_ar_prrt_conditioned&#45;&gt;beta_ar_prrt -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>alpha_ar_prrt_conditioned&#45;&gt;beta_ar_prrt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1777.1,-414.81C1796.73,-406.83 1817.52,-397.47 1836,-387.27 1841.61,-384.18 1847.3,-380.68 1852.86,-377.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1854.51,-380.14 1860.81,-371.62 1850.57,-374.35 1854.51,-380.14\"/>\n",
       "</g>\n",
       "<!-- weight_prrt_prior -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>weight_prrt_prior</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2098\" cy=\"-226.07\" rx=\"80.79\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2098\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_prrt_prior</text>\n",
       "<text text-anchor=\"middle\" x=\"2098\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2098\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_prrt_prior&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>weight_prrt_prior&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2039.13,-198.96C2027.04,-194.39 2014.25,-190.1 2002,-187 1850.68,-148.67 1668.74,-133.63 1563.62,-127.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1564.13,-124.37 1553.96,-127.33 1563.76,-131.36 1564.13,-124.37\"/>\n",
       "</g>\n",
       "<!-- sigma_ar -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>sigma_ar</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1881\" cy=\"-545.59\" rx=\"56.92\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-555.91\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sigma_ar</text>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-540.16\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-524.41\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- sigma_ar&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>sigma_ar&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1924.25,-519.89C1941.54,-507.51 1959.56,-490.8 1969,-470.52 2000.76,-402.27 2000.81,-369.37 1969,-301.14 1935.65,-229.62 1904.52,-220.34 1833,-187 1746.84,-146.84 1638.01,-132.28 1563.49,-127.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1564.01,-123.61 1553.81,-126.46 1563.56,-130.6 1564.01,-123.61\"/>\n",
       "</g>\n",
       "<!-- sigma_ar&#45;&gt;alpha_ar_prrt_conditioned -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>sigma_ar&#45;&gt;alpha_ar_prrt_conditioned</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1837.43,-520.21C1813.91,-507.06 1784.58,-490.65 1759.14,-476.42\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1760.99,-473.45 1750.56,-471.62 1757.58,-479.55 1760.99,-473.45\"/>\n",
       "</g>\n",
       "<!-- beta_ar_prrt_conditioned -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>beta_ar_prrt_conditioned</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1960.25,-470.52 1801.75,-470.52 1801.75,-415.27 1960.25,-415.27 1960.25,-470.52\"/>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-453.22\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">beta_ar_prrt_conditioned</text>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-437.47\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"1881\" y=\"-421.72\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Deterministic</text>\n",
       "</g>\n",
       "<!-- sigma_ar&#45;&gt;beta_ar_prrt_conditioned -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>sigma_ar&#45;&gt;beta_ar_prrt_conditioned</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1881,-506.23C1881,-498.41 1881,-490.17 1881,-482.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1884.5,-482.4 1881,-472.4 1877.5,-482.4 1884.5,-482.4\"/>\n",
       "</g>\n",
       "<!-- weight_oir_prior -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>weight_oir_prior</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2278\" cy=\"-226.07\" rx=\"77.07\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2278\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_oir_prior</text>\n",
       "<text text-anchor=\"middle\" x=\"2278\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2278\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_oir_prior&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>weight_oir_prior&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2222.76,-198.53C2211.47,-194.02 2199.52,-189.87 2188,-187 2071.97,-158.11 1726.8,-137.19 1563.92,-128.64\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1564.19,-125.15 1554.03,-128.12 1563.83,-132.14 1564.19,-125.15\"/>\n",
       "</g>\n",
       "<!-- weight_tnlr_prior -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>weight_tnlr_prior</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2458\" cy=\"-226.07\" rx=\"79.2\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2458\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_tnlr_prior</text>\n",
       "<text text-anchor=\"middle\" x=\"2458\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2458\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_tnlr_prior&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>weight_tnlr_prior&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2400.76,-198.58C2388.83,-194.01 2376.18,-189.82 2364,-187 2213.58,-152.15 1756.64,-133.53 1564.06,-127.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1564.22,-123.55 1554.11,-126.72 1563.99,-130.55 1564.22,-123.55\"/>\n",
       "</g>\n",
       "<!-- weight_oir_sigma -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>weight_oir_sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2283\" cy=\"-340.2\" rx=\"80.79\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2283\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_oir_sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"2283\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2283\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_oir_sigma&#45;&gt;weight_oir_prior -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>weight_oir_sigma&#45;&gt;weight_oir_prior</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2281.29,-300.93C2280.95,-293.16 2280.58,-284.89 2280.22,-276.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2283.73,-276.9 2279.78,-267.06 2276.73,-277.21 2283.73,-276.9\"/>\n",
       "</g>\n",
       "<!-- weight_prrt_sigma -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>weight_prrt_sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2100\" cy=\"-340.2\" rx=\"84.5\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2100\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_prrt_sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"2100\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2100\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_prrt_sigma&#45;&gt;weight_prrt_prior -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>weight_prrt_sigma&#45;&gt;weight_prrt_prior</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2099.32,-300.93C2099.18,-293.16 2099.03,-284.89 2098.89,-276.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2102.39,-277 2098.71,-267.07 2095.39,-277.13 2102.39,-277\"/>\n",
       "</g>\n",
       "<!-- weight_tnlr_sigma -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>weight_tnlr_sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2465\" cy=\"-340.2\" rx=\"82.91\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2465\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_tnlr_sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"2465\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2465\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_tnlr_sigma&#45;&gt;weight_tnlr_prior -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>weight_tnlr_sigma&#45;&gt;weight_tnlr_prior</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2462.61,-300.93C2462.13,-293.16 2461.61,-284.89 2461.1,-276.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2464.61,-276.83 2460.5,-267.06 2457.63,-277.26 2464.61,-276.83\"/>\n",
       "</g>\n",
       "<!-- weight_cf_prior -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>weight_cf_prior</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2637\" cy=\"-226.07\" rx=\"74.42\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2637\" y=\"-236.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_cf_prior</text>\n",
       "<text text-anchor=\"middle\" x=\"2637\" y=\"-220.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2637\" y=\"-204.89\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_cf_prior&#45;&gt;repository_health_index -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>weight_cf_prior&#45;&gt;repository_health_index</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2582.65,-198.84C2570.81,-194.12 2558.18,-189.82 2546,-187 2359.94,-143.95 1784.02,-129.67 1563.82,-125.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1563.92,-122.2 1553.86,-125.52 1563.8,-129.2 1563.92,-122.2\"/>\n",
       "</g>\n",
       "<!-- weight_cf_sigma -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>weight_cf_sigma</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2644\" cy=\"-340.2\" rx=\"78.14\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"2644\" y=\"-350.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">weight_cf_sigma</text>\n",
       "<text text-anchor=\"middle\" x=\"2644\" y=\"-334.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"2644\" y=\"-319.03\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- weight_cf_sigma&#45;&gt;weight_cf_prior -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>weight_cf_sigma&#45;&gt;weight_cf_prior</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M2641.61,-300.93C2641.13,-293.16 2640.61,-284.89 2640.1,-276.8\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"2643.61,-276.83 2639.5,-267.06 2636.63,-277.26 2643.61,-276.83\"/>\n",
       "</g>\n",
       "<!-- beta_ar_prrt_conditioned&#45;&gt;beta_ar_prrt -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>beta_ar_prrt_conditioned&#45;&gt;beta_ar_prrt</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1886.09,-414.91C1887.53,-407.3 1889.14,-398.77 1890.74,-390.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"1894.13,-391.2 1892.54,-380.73 1887.25,-389.9 1894.13,-391.2\"/>\n",
       "</g>\n",
       "<!-- beta_ar_prrt&#45;&gt;expected_pull_request_resolution_time -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>beta_ar_prrt&#45;&gt;expected_pull_request_resolution_time</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1859.39,-311.03C1851.9,-307.03 1843.94,-303.48 1836,-301.14 1629.54,-240.22 1080.33,-300.55 868,-265.14 856.09,-263.15 843.73,-260.36 831.62,-257.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"832.61,-253.83 822.04,-254.57 830.76,-260.58 832.61,-253.83\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x18908936310>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_graph = pm.model_to_graphviz(tribal_knowledge)\n",
    "\n",
    "# Customize the graph - Example changes\n",
    "model_graph.attr(label='Hierarchical tribal_knowledge Model\\nBayesian Model Visualization')\n",
    "model_graph.attr(fontsize='20', color='blue', fontname=\"Helvetica\")\n",
    "\n",
    "# Node and Edge customizations\n",
    "with model_graph.subgraph() as s:\n",
    "    s.attr(rank='same')\n",
    "    # Customize nodes\n",
    "    s.node_attr.update(color='lightblue2', style='filled', fontname=\"Helvetica\")\n",
    "    # Customize edges\n",
    "    s.edge_attr.update(color='gray', arrowsize='0.5')\n",
    "\n",
    "# Render the graph \n",
    "model_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f8b207c9-e05c-43cb-9114-c4b3e660d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VCB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\tensor\\rewriting\\elemwise.py:1029: UserWarning: Loop fusion failed because the resulting node would exceed the kernel argument limit.\n",
      "  warn(\n",
      "C:\\Users\\VCB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\tensor\\rewriting\\elemwise.py:1029: UserWarning: Loop fusion failed because the resulting node would exceed the kernel argument limit.\n",
      "  warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Numba does not support NumPy `Generator`s",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fiting the model - using nutpie for faster sampling\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tribal_knowledge:\n\u001b[1;32m----> 4\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnuts_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnutpie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:674\u001b[0m, in \u001b[0;36msample\u001b[1;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, NUTS):\n\u001b[0;32m    671\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    672\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel can not be sampled with NUTS alone. Your model is probably not continuous.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    673\u001b[0m         )\n\u001b[1;32m--> 674\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_external_nuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnuts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_accept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitvals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitvals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    690\u001b[0m     step \u001b[38;5;241m=\u001b[39m CompoundStep(step)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:295\u001b[0m, in \u001b[0;36m_sample_external_nuts\u001b[1;34m(sampler, draws, tune, chains, target_accept, random_seed, initvals, model, progressbar, idata_kwargs, nuts_sampler_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idata_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`idata_kwargs` are currently ignored by the nutpie sampler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    294\u001b[0m     )\n\u001b[1;32m--> 295\u001b[0m compiled_model \u001b[38;5;241m=\u001b[39m \u001b[43mnutpie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_pymc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    297\u001b[0m idata \u001b[38;5;241m=\u001b[39m nutpie\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    298\u001b[0m     compiled_model,\n\u001b[0;32m    299\u001b[0m     draws\u001b[38;5;241m=\u001b[39mdraws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnuts_sampler_kwargs,\n\u001b[0;32m    306\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\nutpie\\compile_pymc.py:190\u001b[0m, in \u001b[0;36mcompile_pymc_model\u001b[1;34m(model, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba is not installed in the current environment. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with something like \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmamba install -c conda-forge numba\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand restart your kernel in case you are in an interactive session.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    178\u001b[0m     )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[0;32m    181\u001b[0m (\n\u001b[0;32m    182\u001b[0m     n_dim,\n\u001b[0;32m    183\u001b[0m     n_expanded,\n\u001b[0;32m    184\u001b[0m     logp_fn_pt,\n\u001b[0;32m    185\u001b[0m     logp_fn,\n\u001b[0;32m    186\u001b[0m     expand_fn_pt,\n\u001b[0;32m    187\u001b[0m     expand_fn,\n\u001b[0;32m    188\u001b[0m     shared_expand,\n\u001b[0;32m    189\u001b[0m     shape_info,\n\u001b[1;32m--> 190\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43m_make_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m shared_data \u001b[38;5;241m=\u001b[39m {val\u001b[38;5;241m.\u001b[39mname: val\u001b[38;5;241m.\u001b[39mget_value()\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m logp_fn_pt\u001b[38;5;241m.\u001b[39mget_shared()}\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m shared_data\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\nutpie\\compile_pymc.py:371\u001b[0m, in \u001b[0;36m_make_functions\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    368\u001b[0m num_expanded \u001b[38;5;241m=\u001b[39m count\n\u001b[0;32m    370\u001b[0m allvars \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mconcatenate([joined, \u001b[38;5;241m*\u001b[39m[var\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m remaining_rvs]])\n\u001b[1;32m--> 371\u001b[0m expand_fn_pt \u001b[38;5;241m=\u001b[39m \u001b[43mpytensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mallvars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgivens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplacements\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUMBA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m expand_fn \u001b[38;5;241m=\u001b[39m expand_fn_pt\u001b[38;5;241m.\u001b[39mvm\u001b[38;5;241m.\u001b[39mjit_fn\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    380\u001b[0m     num_free_vars,\n\u001b[0;32m    381\u001b[0m     num_expanded,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m     (all_names, all_slices, all_shapes),\n\u001b[0;32m    388\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\compile\\function\\__init__.py:315\u001b[0m, in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    309\u001b[0m     fn \u001b[38;5;241m=\u001b[39m orig_function(\n\u001b[0;32m    310\u001b[0m         inputs, outputs, mode\u001b[38;5;241m=\u001b[39mmode, accept_inplace\u001b[38;5;241m=\u001b[39maccept_inplace, name\u001b[38;5;241m=\u001b[39mname\n\u001b[0;32m    311\u001b[0m     )\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# note: pfunc will also call orig_function -- orig_function is\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m#      a choke point that all compilation must pass through\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     fn \u001b[38;5;241m=\u001b[39m \u001b[43mpfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mupdates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgivens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgivens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_default_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_default_updates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrebuild_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrebuild_strict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_input_downcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_input_downcast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_unused_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_unused_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\compile\\function\\pfunc.py:469\u001b[0m, in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys, fgraph)\u001b[0m\n\u001b[0;32m    455\u001b[0m     profile \u001b[38;5;241m=\u001b[39m ProfileStats(message\u001b[38;5;241m=\u001b[39mprofile)\n\u001b[0;32m    457\u001b[0m inputs, cloned_outputs \u001b[38;5;241m=\u001b[39m construct_pfunc_ins_and_outs(\n\u001b[0;32m    458\u001b[0m     params,\n\u001b[0;32m    459\u001b[0m     outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m     fgraph\u001b[38;5;241m=\u001b[39mfgraph,\n\u001b[0;32m    467\u001b[0m )\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_inplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_inplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_unused_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_unused_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\compile\\function\\types.py:1762\u001b[0m, in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys, fgraph)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     m \u001b[38;5;241m=\u001b[39m Maker(\n\u001b[0;32m   1751\u001b[0m         inputs,\n\u001b[0;32m   1752\u001b[0m         outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1759\u001b[0m         fgraph\u001b[38;5;241m=\u001b[39mfgraph,\n\u001b[0;32m   1760\u001b[0m     )\n\u001b[0;32m   1761\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mchange_flags(compute_test_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1762\u001b[0m         fn \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m profile \u001b[38;5;129;01mand\u001b[39;00m fn:\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\compile\\function\\types.py:1654\u001b[0m, in \u001b[0;36mFunctionMaker.create\u001b[1;34m(self, input_storage, storage_map)\u001b[0m\n\u001b[0;32m   1651\u001b[0m start_import_time \u001b[38;5;241m=\u001b[39m pytensor\u001b[38;5;241m.\u001b[39mlink\u001b[38;5;241m.\u001b[39mc\u001b[38;5;241m.\u001b[39mcmodule\u001b[38;5;241m.\u001b[39mimport_time\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config\u001b[38;5;241m.\u001b[39mchange_flags(traceback__limit\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtraceback__compile_limit):\n\u001b[1;32m-> 1654\u001b[0m     _fn, _i, _o \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_thunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_storage_lists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_map\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1658\u001b[0m end_linker \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m   1660\u001b[0m linker_time \u001b[38;5;241m=\u001b[39m end_linker \u001b[38;5;241m-\u001b[39m start_linker\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\basic.py:245\u001b[0m, in \u001b[0;36mLocalLinker.make_thunk\u001b[1;34m(self, input_storage, output_storage, storage_map, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_thunk\u001b[39m(\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    240\u001b[0m     input_storage: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputStorageType\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    244\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasicThunkType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputStorageType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputStorageType\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\basic.py:688\u001b[0m, in \u001b[0;36mJITLinker.make_all\u001b[1;34m(self, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m storage_map:\n\u001b[0;32m    686\u001b[0m     compute_map[k] \u001b[38;5;241m=\u001b[39m [k\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 688\u001b[0m thunks, nodes, jit_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_jitable_thunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_map\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    692\u001b[0m computed, last_user \u001b[38;5;241m=\u001b[39m gc_helper(nodes)\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallow_gc:\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\basic.py:638\u001b[0m, in \u001b[0;36mJITLinker.create_jitable_thunk\u001b[1;34m(self, compute_map, order, input_storage, output_storage, storage_map)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;66;03m# This is a bit hackish, but we only return one of the output nodes\u001b[39;00m\n\u001b[0;32m    636\u001b[0m output_nodes \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfgraph\u001b[38;5;241m.\u001b[39moutputs \u001b[38;5;28;01mif\u001b[39;00m o\u001b[38;5;241m.\u001b[39mowner \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m][:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 638\u001b[0m converted_fgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfgraph_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m thunk_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_thunk_inputs(storage_map)\n\u001b[0;32m    648\u001b[0m thunks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\numba\\linker.py:27\u001b[0m, in \u001b[0;36mNumbaLinker.fgraph_convert\u001b[1;34m(self, fgraph, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfgraph_convert\u001b[39m(\u001b[38;5;28mself\u001b[39m, fgraph, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numba_funcify\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnumba_funcify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\numba\\dispatch\\basic.py:448\u001b[0m, in \u001b[0;36mnumba_funcify_FunctionGraph\u001b[1;34m(fgraph, node, fgraph_name, **kwargs)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;129m@numba_funcify\u001b[39m\u001b[38;5;241m.\u001b[39mregister(FunctionGraph)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumba_funcify_FunctionGraph\u001b[39m(\n\u001b[0;32m    443\u001b[0m     fgraph,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    447\u001b[0m ):\n\u001b[1;32m--> 448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfgraph_to_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumba_funcify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_conversion_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumba_typify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfgraph_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfgraph_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\utils.py:734\u001b[0m, in \u001b[0;36mfgraph_to_python\u001b[1;34m(fgraph, op_conversion_fn, type_conversion_fn, order, storage_map, fgraph_name, global_env, local_env, get_name_for_object, squeeze_output, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m body_assigns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m order:\n\u001b[1;32m--> 734\u001b[0m     compiled_func \u001b[38;5;241m=\u001b[39m \u001b[43mop_conversion_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;66;03m# Create a local alias with a unique name\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     local_compiled_func_name \u001b[38;5;241m=\u001b[39m unique_name(compiled_func)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:909\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    907\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\numba\\dispatch\\random.py:218\u001b[0m, in \u001b[0;36mnumba_funcify_RandomVariable\u001b[1;34m(op, node, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m name \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    216\u001b[0m np_random_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(np\u001b[38;5;241m.\u001b[39mrandom, name)\n\u001b[1;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_numba_random_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_random_func\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\link\\numba\\dispatch\\random.py:99\u001b[0m, in \u001b[0;36mmake_numba_random_fn\u001b[1;34m(node, np_random_func)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create Numba implementations for existing Numba-supported ``np.random`` functions.\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03mThe functions generated here add parameter broadcasting and the ``size``\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03margument to the Numba-supported scalar ``np.random`` functions.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtype, RandomStateType):\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumba does not support NumPy `Generator`s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m tuple_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(get_vector_length(node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    102\u001b[0m size_dims \u001b[38;5;241m=\u001b[39m tuple_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mmax\u001b[39m(i\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m3\u001b[39m:])\n",
      "\u001b[1;31mTypeError\u001b[0m: Numba does not support NumPy `Generator`s"
     ]
    }
   ],
   "source": [
    "# Fiting the model - using nutpie for faster sampling\n",
    "\n",
    "with tribal_knowledge:\n",
    "    trace = pm.sample(500, tune=500, return_inferencedata=True, target_accept=0.95, nuts_sampler=\"nutpie\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d0d40-f642-4632-875b-4a9395e2491c",
   "metadata": {},
   "source": [
    "### Results and Interpretation\n",
    "\n",
    "This section guides you through extracting and interpreting the results of the hierarchical Bayesian model applied to programming language usage within an organisation. We'll focus on understanding the impact of various factors on language usage, visualise distributions across Owners and repositories, and identify potential outliers or areas of risk.\n",
    "\n",
    "#### Identifying Outliers Based on Credible Intervals\n",
    "\n",
    "Let's identify repositories whose observed language usage falls outside the credible intervals predicted by our model. These could be considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a429bb5-480d-430f-9fa3-4b8fee98a779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform posterior predictive checks, which generate posterior predictive samples given a trace and a model:\n",
    "\n",
    "with tribal_knowledge:\n",
    "    ppc = pm.sample_posterior_predictive(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0894de-6a1d-414c-a9e2-bf71195da313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the posterior_predictive group\n",
    "posterior_predictive = ppc.posterior_predictive\n",
    "summary_stats = az.summary(posterior_predictive, var_names=['language_count_obs'], hdi_prob=0.95)\n",
    "\n",
    "# Assume byte_count is a list/array with the observed byte counts ordered as in your model\n",
    "# This approach requires the model's indices to match the 'Repo_Lang_Key_codes' in 'df'\n",
    "outliers = []\n",
    "\n",
    "# Adding columns for CI lower, CI upper, and outlier flag directly to the df\n",
    "df['CI_Lower'] = pd.NA\n",
    "df['CI_Upper'] = pd.NA\n",
    "df['Is_Outlier'] = False\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    repo_lang_key_code = row['Repo_Lang_Key_codes']\n",
    "    ci_lower = summary_stats.loc[f\"language_count_obs[{repo_lang_key_code}]\", \"hdi_2.5%\"]\n",
    "    ci_upper = summary_stats.loc[f\"language_count_obs[{repo_lang_key_code}]\", \"hdi_97.5%\"]\n",
    "    \n",
    "    # Update the DataFrame directly with CI values\n",
    "    df.at[index, 'CI_Lower'] = ci_lower\n",
    "    df.at[index, 'CI_Upper'] = ci_upper\n",
    "    \n",
    "    # Check if the current row's ByteCount is an outlier\n",
    "    is_outlier = row['ByteCount'] < ci_lower or row['ByteCount'] > ci_upper\n",
    "    df.at[index, 'Is_Outlier'] = is_outlier\n",
    "\n",
    "\n",
    "print(f'Number of outlier Repo-Language combinations:, {df[\"Is_Outlier\"].sum()}')\n",
    "\n",
    "# Extracting detailed information for each outlier repository\n",
    "# Now using the 'Is Outlier' flag directly instead of checking membership in the 'outliers' list\n",
    "outlier_repos_info = df[df['Is_Outlier']].drop_duplicates(subset=['Unique_Repo'])\n",
    "\n",
    "# Printing detailed information for outlier repositories including CI values\n",
    "print(\"Outlier Repositories Detailed Information:\")\n",
    "print(outlier_repos_info[['Owner', 'Repository', 'Unique_Repo', 'ByteCount', 'CI_Lower', 'CI_Upper', 'Is_Outlier']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8ae52-2230-4645-97e4-0da2494cc1f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_results = False\n",
    "if write_results:\n",
    "    ## Write file and trace to ./data/... with git commit-ID for checks\n",
    "    commit_id = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).strip().decode('utf-8')\n",
    "    print(commit_id)\n",
    "    \n",
    "    # Write data with confidence regions:\n",
    "    outlier_repos_info[['Owner', 'Repository', 'Unique_Repo', 'ByteCount', 'CI_Lower', 'CI_Upper', 'Is_Outlier']].to_csv(path_or_buf=f'data/outlier_language_repo_{commit_id}.csv', index=False)\n",
    "    print(\"Outlier file written\")\n",
    "    \n",
    "    # Write trace data\n",
    "    # We use NetCDF format here as pickle is unsafe\n",
    "    filename_with_commit = f\"trace_data_{commit_id}.nc\"\n",
    "    trace.to_netcdf(f'data/{filename_with_commit}')\n",
    "    print(\"Inference object written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf5bac-787f-4d0e-b43b-c0e456c26833",
   "metadata": {},
   "source": [
    "#### Deep Dive into Outliers\n",
    "With the identified outliers, a deeper investigation into these specific repositories or Owners can provide insights into why they deviate from the model's expectations.\n",
    "Lets look deeper into teh posterior distributions to figure out why this has been flagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78718509-34df-4a0c-bbe0-143070986882",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outlier_repos_info[['Unique_Repo', 'Is_Outlier']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114e475-4a16-4e2c-aac6-5b325a6c1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis = LanguagePosteriorAnalysis(df, ppc, {'Owner': 'Apache', 'Repository': 'zookeeper'})\n",
    "outlier_report = analysis.calculate_outliers()\n",
    "print(outlier_report)\n",
    "# plot distributions \n",
    "analysis.plot_all_languages_posterior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d7e907-ec33-4744-b4ce-7fe76bf3d50d",
   "metadata": {},
   "source": [
    "#### Example write up\n",
    "\n",
    "Our comprehensive analysis of programming language byte counts within Owner_115_Repo_3, leveraging Hierarchical Bayesian Modelling (HBM), has illuminated various outlier scenarios across various programming languages. Notably, many languages, including Haskell, were identified as outliers based on their log-byte counts' 95% credible intervals. This widespread classification of outlier status points towards potential areas for model improvement and prompts a re-evaluation of our data generation and representation assumptions.\n",
    "\n",
    "The observed log byte counts for several languages diverge markedly from the expected ranges predicted by our model. For example, Haskell and PowerShell exhibit notably lower log byte counts than anticipated, suggesting their utilization in the repository might be for highly specialized or concise functionalities not captured by our current model structure.\n",
    "\n",
    "This analysis opens several avenues for deeper exploration:\n",
    "\n",
    "- **Model Refinement:** The extensive identification of outliers underscores the need for model refinement. This could involve revisiting our model's assumptions, incorporating additional covariates that might explain the observed variability, or exploring more flexible distributions to better accommodate the diversity in programming language usage patterns.\n",
    "\n",
    "- **Data Generation Compatibility:** The disparity between the model's expectations and the observed data highlights the importance of closely examining the data generation process. Ensuring that the process accurately reflects realistic programming language distributions and repository characteristics is crucial for model calibration.\n",
    "\n",
    "- **Focused Investigation:** Languages' outlier status, particularly those with significant deviations like Haskell and PowerShell, invites focused investigation into their roles within the repository. Understanding the context and rationale behind their usage can provide valuable insights into Owner-specific practices and potential areas for optimization.\n",
    "\n",
    "-  **Organizational Strategy:** Beyond model adjustments, these findings signal an opportunity to assess the broader organizational strategy regarding technology stack diversity, coding practices, and repository management. Identifying whether the observed patterns align with strategic goals or necessitate adjustments is essential for fostering a productive and innovative development environment.\n",
    "This example highlights the importance of Hierarchical Bayesian Modelling (HBM) in detecting outliers and providing context in a multilevel data structure. By using this approach, we can improve our understanding of language usage patterns, which can inform strategic decisions around technology adoption, Owner management, and developer training.\n",
    "\n",
    "Although our analysis revealed discrepancies between our model's predictions and observed data, it provides an opportunity for reflection and growth. We can enhance our understanding of programming language dynamics within our Owners by addressing these discrepancies through model refinement and strategic alignment. This will align our technological practices with organizational objectives, leading to informed decision-making and strategic development initiatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ce3e8-c17d-4b31-8a12-e3069b7144eb",
   "metadata": {},
   "source": [
    "## EXTENSIONS\n",
    "\n",
    "This is a very trivial modelling example and can be refined and updated to include far more variables to mutually understand a repo, i.e time since last commit for staleness, or active dependencies, total number of committers etc, etc. \n",
    "\n",
    "A novel and perhaps useful extension might be saving the trace of the model and creatin a streamlit app enabling ad hoc investigation, this could be especially powerful with the inclusion of the `do` operator that has now been introduced for what-if analysis (https://www.pymc.io/Owners/examples/en/latest/causal_inference/interventional_distribution.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d423a-5faf-40b1-8d14-bc5664b62076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ec388-2043-4797-adf7-59aec3c945e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
