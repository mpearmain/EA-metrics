{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d3bdff101d217",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Modeling to assess tribal knowledge\n",
    "\n",
    "In this analysis, we will try to create a methodology and data-driven metric for identifying potential technological risks within an organization's coding protocols. We'll examine how programming languages are utilised across various projects and repositories, similar to those found in code repositories such as GitHub,  and leverage Hierarchical Bayesian Modelling (HBM) for multi-level data analysis. \n",
    "\n",
    "HBM effectively captures project-specific variances and overall project trends, providing a nuanced \"risk\" metric for Enterprise Architecture. This enables organisations to identify potential knowledge silos and make strategic decisions to enhance project continuity and organisational adaptability.\n",
    "\n",
    "By analysing language usage across different organisational levels and integrating uncertainty, HBM aims to expose pockets of siloed tribal knowledge (in this example, via a proxy of languages used, but can easily be extended to accommodate other features such a #of commits, time since last commit, total commits, etc., etc.), which is crucial for identifying hidden risks within the architectural framework. This analysis uncovers potential vulnerabilities and compares language usage at repository and project scales against wider organisational patterns. These comparative insights are critical, revealing when a technology may seem insignificant in isolation emerges as a considerable risk in the broader organisational context due to limited expertise or exposure. This comprehensive examination ensures that technology decisions are made with a strategic perspective, reinforcing organisational resilience in the face of technological evolution.\n",
    "\n",
    "For a more concrete example, consider a scenario where an organisation's repository primarily uses Haskell, a language not commonly used in broader enterprise contexts. Hierarchical Bayesian Modelling evaluates the risk by scrutinising Haskell's application within the repository, its relevance to the project, and its organisational prevalence. This comprehensive assessment ascertains the alignment of Haskell's use with the enterprise's technological trajectory and knowledge base, guiding strategic architectural decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee99bdbedd31ef3",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "To start building a hierarchical Bayesian model using PyMC3 based on your JSON data, you'll first need to parse the data to extract the relevant information for modelling. This involves aggregating language usage across repositories and projects. After that, we define a hierarchical model that captures the variability within repositories and commonalities across projects.\n",
    "\n",
    "* Data Preparation: Aggregate the language bytes for each language across all repositories and projects.\n",
    "* Model Definition: Define a hierarchical model in Pymc, using project-level priors influencing repository-level distributions.\n",
    "* Inference: Use MCMC provided by pymc to sample from the posterior distribution.\n",
    "* Analysis: Analyze the posterior distributions to identify languages with usage outside the credible regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64929205d7d6c912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:12.460170Z",
     "start_time": "2024-02-27T04:55:12.456226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import graphviz\n",
    "import arviz as az\n",
    "import pprint\n",
    "\n",
    "from utils import load_data, json2pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0862-2954-49f3-a206-e69b84cddea7",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "The first step is to run the `generate_dummy_data.py` file to make sure we have data to play around with, the generated dummy data is similar to what you might pull from GitHub's REST API for repository languages https://docs.github.com/en/rest/repos/repos?apiVersion=2022-11-28#list-repository-languages\n",
    "\n",
    "```GitHub CLI api\n",
    "https://cli.github.com/manual/gh_api\n",
    "\n",
    "gh api \\\n",
    "  -H \"Accept: application/vnd.github+json\" \\\n",
    "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
    "  /repos/OWNER/REPO/languages\n",
    "\n",
    "Example Response:\n",
    "{\n",
    "  \"C\": 78769,\n",
    "  \"Python\": 7769\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "From here, we can load and transform the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc6912-e1b0-4ff3-bd41-da2c962a92af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:16.064859Z",
     "start_time": "2024-02-27T04:55:16.057668Z"
    }
   },
   "outputs": [],
   "source": [
    "df_json = load_data(\"data/dummy_language_data.json\")\n",
    "# Prety print some Projects and Repos randomly to visualise the data\n",
    "NUM_PROJECTS = 1\n",
    "first_N_projects = {k: df_json[k] for k in list(df_json)[:NUM_PROJECTS]}\n",
    "pp = pprint.PrettyPrinter(depth=3)\n",
    "pp.pprint(first_N_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479e6ad-ad19-4f50-bc99-495b84ea55e2",
   "metadata": {},
   "source": [
    "Let's flip this into a normal dataset we are used to, and and a new variable to log transform the byte count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a57fbc-6bc4-4c72-bb1f-858e38ba2b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:19.331032Z",
     "start_time": "2024-02-27T04:55:19.315700Z"
    }
   },
   "outputs": [],
   "source": [
    "df=json2pandas(df_json)\n",
    "# Using pandas to calculate the logarithm of the ByteCount column\n",
    "df['logByteCount'] = np.log(df['ByteCount'])\n",
    "# Creating a new column Project_Repo by concatenating Project and Repository columns\n",
    "df['Project_Repo'] = df['Project'] + \"_\" + df['Repository']\n",
    "\n",
    "columns_to_encode = ['Project', 'Repository', 'Language', 'Project_Repo']\n",
    "\n",
    "# Loop over the columns to encode\n",
    "for column_name in columns_to_encode:\n",
    "    # Cast each column to Categorical and add it as a new column with a suffix '_codes'\n",
    "    df[f'{column_name}_codes'] = df[column_name].astype('category').cat.codes\n",
    "\n",
    "    \n",
    "print(df.head(n=20))\n",
    "print(\"Total number of projects:\", df['Project'].unique().shape)\n",
    "print(\"Total number of projects repo ids:\", df['Project_Repo_codes'].unique().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fe316-0f6a-4b26-8623-8f30ecb6b171",
   "metadata": {},
   "source": [
    "## Hierarchical Model Specification\n",
    "\n",
    "This section introduces Hierarchical Bayesian Modelling (HBM) principles and their application in structuring complex, multi-level datasets, such as those encountered in evaluating technological risks within coding languages.\n",
    "\n",
    "#### Introduction to Hierarchical Bayesian Modelling\n",
    "Hierarchical Bayesian Modelling is a statistical framework that enables data analysis across different levels of hierarchy by integrating the variability within individual units (such as repositories) and the commonalities across groups (such as projects, domains, or departments). Bayes' theorem is at the core of Bayesian inference, which updates the probability for a hypothesis as more evidence becomes available. One of the critical concepts in HBM is exchangeability, which implies that data points are probabilistically symmetrical. This makes it suitable for modelling data that doesn't have a natural ordering or grouping but is considered identically distributed given some unknown parameters.\n",
    "\n",
    "When we analyse the usage of programming languages, we're looking at a hierarchical model structure with multiple layers. Given that the `ByteCount` variable represents the number of bytes of code for a given language in a repository, it essentially counts data (albeit at a potentially large scale that we might want to log-transform later). Count data is often modelled using distributions specifically suited to non-negative integer values, such as the Negative Binomial, leading us to represent our model as a Hierarchical Beta-Negative Binomial Model.\n",
    "\n",
    "These layers represent language usage within repositories, which are nested within projects.\n",
    "\n",
    "- **Level 1 - Repository-Level Likelihood:** At this level, we describe the observed data, such as the amount of code written in each language within a repository, using a likelihood function.\n",
    "\n",
    "  $$ P(Language_{ij} | \\theta_{ij}) \\sim NegativeBinomial(\\mu_{ij}, \\alpha) $$\n",
    "\n",
    "  Here, \\( \\theta_{ij} \\) represents the expected byte count for language usage, where \\( i \\) denotes the repository and \\( j \\) the language. \\( \\mu_{ij} \\) is the mean parameter and \\( \\alpha \\) is the dispersion parameter of the Negative Binomial distribution.\n",
    "\n",
    "- **Level 2 - Project-Level Priors:** As we move up to the project level, parameters from the repository level are considered uncertain and are described by priors.\n",
    "\n",
    "  $$ \\mu_{ij} | \\mu_i, \\kappa_i \\sim Beta(\\mu_i \\kappa_i, (1 - \\mu_i) \\kappa_i) $$\n",
    "\n",
    "  The Beta distribution parameters \\( \\mu_i \\) and \\( \\kappa_i \\) represent the expected language usage and variability within a project, influencing the mean parameter \\( \\mu_{ij} \\) of the Negative Binomial distribution at the repository level.\n",
    "\n",
    "- **Level 3 - Organisational-Level Hyperpriors:** At the organisational level, we look at the broader patterns in language usage across all projects.\n",
    "\n",
    "  $$ \\mu_i \\sim Beta(a_{\\mu}, b_{\\mu}) $$\n",
    "  $$ \\kappa_i \\sim Gamma(a_{\\kappa}, b_{\\kappa}) $$\n",
    "\n",
    "  Hyperpriors for \\( \\mu_i \\) and \\( \\kappa_i \\) reflect our assumptions about these patterns before analysing the data.\n",
    "\n",
    "This hierarchical approach allows for a detailed analysis of organisational language usage. We're not just modelling individual repositories but also capturing trends across projects and the entire organisation.\n",
    "\n",
    "#### Explanation of Model Parameters and Priors\n",
    "\n",
    "Hierarchical Bayesian Modelling (HBM) views parameters as distributions, known as priors. Priors represent our initial beliefs or understanding about the parameters before examining the data. When dealing with count data, such as the count of bytes of code in different programming languages within repositories, overdispersion is often observed in such data. A Negative Binomial distribution can account for this overdispersion.\n",
    "\n",
    "The model includes hyperparameters, such as the mean parameter  \\($ \\mu $\\)  for the Negative Binomial distribution. These hyperparameters introduce a layer of variability accounting for differences within repositories and across projects. Moreover, these hyperparameters are informed by higher-level distributions, or hyperpriors. For example, a Beta distribution might be used to model project-level variability, influencing the Negative Binomial's mean parameter \\( $\\mu $\\), while a Gamma distribution is applied to the dispersion parameter \\($ \\alpha $\\), relating to the variability of byte counts.\n",
    "\n",
    "Employing this type of hierarchical model enables the refinement of our initial beliefs in light of collected data, leading to a deeper and more nuanced understanding of an organisation's coding practices. The adaptability of this model to new information enhances the precision of our insights, effectively capturing both the average and the variance of language usage within the complex hierarchical structure of repositories and projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666c696-acdd-48d5-ba48-dfd4480b6f0d",
   "metadata": {},
   "source": [
    "\n",
    "## Transitioning to Posterior Distributions\n",
    "\n",
    "### From Theory to Practice: The Role of Posterior Distributions\n",
    "\n",
    "With our model parameters defined and their priors set, the next step in Bayesian analysis is to update these beliefs with observed data. This is where the posterior distribution comes into play.\n",
    "\n",
    "\n",
    "#### What is the posterior?\n",
    "\n",
    "In the hierarchical Bayesian modelling (HBM) context, the posterior distribution is the updated belief about our model's parameters after considering the observed data. It combines our prior beliefs (the priors) and the evidence from the data (the likelihood). Mathematically, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(\\theta | data) \\propto P(data | \\theta) \\times P(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ P(\\theta | data) $\\) is the posterior distribution of the parameters \\($ \\theta $\\).\n",
    "- \\($ P(data | \\theta) $\\) is the likelihood of the data given the parameters.\n",
    "- \\($ P(\\theta) $\\) is the prior distribution of the parameters.\n",
    "\n",
    "After observing the data, the posterior distribution provides a range of likely values for the parameters, which is crucial for making informed decisions.\n",
    "\n",
    "\n",
    "### Practical Implications of Posterior Analysis in Hierarchical Bayesian Modelling\n",
    "\n",
    "Through the lens of Hierarchical Bayesian Modelling, the posterior distribution becomes a beacon, illuminating the path to understanding and action within an organisation's coding practices.\n",
    "\n",
    "\n",
    "### Informing Strategic Decision-Making\n",
    "\n",
    "The power of posterior analysis extends beyond diagnostics; it informs strategic resource allocation and risk management:\n",
    "\n",
    "- **Credible Intervals**: The precision of parameter estimates, reflected in the credible intervals of the posterior distribution, directs our focus to areas where additional data collection or deeper investigation may be warranted.\n",
    "\n",
    "- **Outlier Detection**: Spotting outliers within posterior distributions alerts us to unconventional language usage patterns. These could represent areas of innovation warranting further exploration or potential risks if the languages in question lack broad support.\n",
    "\n",
    "- **Strategic Resource Allocation**: Insights gained from posterior distributions enable informed decisions on resource allocation—be it for targeted training programmes, strategic hiring to build expertise in underutilised languages, or investment in technology stacks that promise to align with and propel the organisation's strategic objectives.\n",
    "\n",
    "Interpreting the posterior distributions derived from our hierarchical model does more than just enhance our understanding of language usage; it equips us to forecast, plan, and foster a coding environment that is both efficient and resilient to future challenges.\n",
    "\n",
    "### Uncovering Knowledge Silos\n",
    "\n",
    "The posterior distributions for language usage within repositories serve as a diagnostic tool, revealing languages that are disproportionately relied upon. Anomalies in these distributions may signal the existence of knowledge silos, suggesting areas where diversification and training could be beneficial. By identifying these silos, we can proactively address potential bottlenecks in knowledge transfer and code maintenance.\n",
    "\n",
    "### Assessing Project-Level Variability\n",
    "\n",
    "The consistency of coding practices across repositories within projects is characterised by project-level hyperparameters. When significant variability is observed in these posterior distributions, it may reflect fragmented coding practices that could undermine team collaboration and project efficiency. This insight drives us to review and possibly revise coding standards, ensuring that practices are aligned and conducive to project success.\n",
    "\n",
    "### Evaluating Organisational Coding Norms\n",
    "\n",
    "At the highest organisational level, posterior distributions offer a macro perspective of coding culture and norms. Deviations in these distributions can reveal organisational preferences or aversions towards specific languages. Understanding these trends is critical for shaping future strategies in technology adoption, capability development, and training initiatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280421-54eb-4c68-8cb1-f28ebb96f6cb",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- Implementing the HBM using PyMC3\n",
    "- Defining the model in PyMC3\n",
    "- Setting up the priors for each level of the hierarchy\n",
    "- Incorporating the data into the model\n",
    "- Model fitting (e.g., using MCMC methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc360bd-a6f5-49ab-a82b-9a8379efa9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T06:23:43.619461Z",
     "start_time": "2024-02-27T06:23:43.590571Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame and necessary imports are done\n",
    "n_projects = df['Project_codes'].nunique()\n",
    "n_repos = df['Repository_codes'].nunique()\n",
    "n_project_repos = df['Project_Repo_codes'].nunique()\n",
    "n_languages = df['Language_codes'].nunique()\n",
    "\n",
    "# Calculate average byte count per language as a starting point\n",
    "avg_byte_per_language = df.groupby('Language_codes')['ByteCount'].mean().reset_index()\n",
    "avg_byte_per_language['logByte'] = np.log(avg_byte_per_language['ByteCount'])\n",
    "\n",
    "# Calculate average byte count per project\n",
    "avg_byte_per_project = df.groupby('Project_codes')['ByteCount'].mean().reset_index()\n",
    "avg_byte_per_project['logByte'] = np.log(avg_byte_per_project['ByteCount'])\n",
    "\n",
    "# Calculate average byte count per repository\n",
    "avg_byte_per_repo = df.groupby('Repository_codes')['ByteCount'].mean().reset_index()\n",
    "avg_byte_per_repo['logByte'] = np.log(avg_byte_per_repo['ByteCount'])\n",
    "\n",
    "# Calculate average byte count per project per repository\n",
    "avg_byte_per_projrepo = df.groupby('Project_Repo_codes')['ByteCount'].mean().reset_index()\n",
    "avg_byte_per_projrepo['logByte'] = np.log(avg_byte_per_projrepo['ByteCount'])\n",
    "\n",
    "project_idx = df['Project_codes'].values\n",
    "repo_idx = df['Repository_codes'].values\n",
    "languages_idx = df['Language_codes'].values\n",
    "byte_count = df['ByteCount'].values\n",
    "\n",
    "# Outside the model, verify shapes and values directly in Python\n",
    "print(f'n_languages: {n_languages}, n_projects: {n_projects}, n_repos: {n_repos}')\n",
    "print(f'avg_byte_per_language: {avg_byte_per_language.head(5)}\\n',  \n",
    "      f'avg_byte_per_project: {avg_byte_per_project.head(5)}\\n', \n",
    "      f'avg_byte_per_repo: {avg_byte_per_repo.head(5)}\\n', \n",
    "      f'avg_byte_per_projrepo: {avg_byte_per_projrepo.head(5)}\\n\\n')\n",
    "print(f'project_idx shape: {project_idx.shape}, repo_idx shape: {repo_idx.shape}, languages_idx shape: {languages_idx.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b882230e-57d8-4bf4-8cdd-4ec72954db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor\n",
    "\n",
    "# Example of setting a PyTensor configuration option for debugging\n",
    "pytensor.config.exception_verbosity = 'high'\n",
    "\n",
    "# Build the model\n",
    "with pm.Model() as language_usage_model:\n",
    "    # Define organisational-level hyperpriors that influence project-level parameters\n",
    "    # These represent our assumptions about the variability across all projects\n",
    "    a_mu = pm.Gamma('a_mu', alpha=1.0, beta=1.0, initval=2.0)\n",
    "    b_mu = pm.Gamma('b_mu', alpha=1.0, beta=1.0, initval=2.0)\n",
    "    a_kappa = pm.Gamma('a_kappa', alpha=1.0, beta=0.1, initval=2.0)\n",
    "    b_kappa = pm.Gamma('b_kappa', alpha=1.0, beta=0.1, initval=2.0)\n",
    "\n",
    "    # Language-Level Priors\n",
    "    # Assuming each language has an associated effect on the byte counts\n",
    "    language_effect = pm.Normal('language_effect', mu=0, sigma=1, shape=n_languages)\n",
    "    \n",
    "    # Define project-level priors that capture the mean and variability of language usage within each project\n",
    "    # 'mu_project' represents the expected proportion of language usage within projects\n",
    "    # 'kappa_project' captures the variability of language usage within projects\n",
    "    mu_project = pm.Beta('mu_project', alpha=a_mu, beta=b_mu, shape=n_projects)\n",
    "    kappa_project = pm.Gamma('kappa_project', alpha=a_kappa, beta=b_kappa, shape=n_projects)\n",
    "    \n",
    "    \n",
    "    # Define repository-level effects influenced by their respective projects 'theta_repo' represents the expected\n",
    "    # Proportion of language usage within each repository, influenced by the project to which it belongs.\n",
    "    theta_repo = pm.Beta('theta_repo', alpha=mu_project[project_idx] * kappa_project[project_idx], \n",
    "                         beta=(1 - mu_project[project_idx]) * kappa_project[project_idx], \n",
    "                         shape=n_repos)\n",
    "    \n",
    "    # Repository-Level Likelihood adjusted for language effect\n",
    "    # Here, we assume that the byte counts are influenced by both the repository effect and the language effect\n",
    "    mu_repo_lang = theta_repo[repo_idx] + language_effect[languages_idx]\n",
    "        \n",
    "    # Define the dispersion parameter for the Negative Binomial distribution to account for overdispersion in the byte counts 'dispersion_factor' controls the variance of the\n",
    "    # Negative Binomial distribution independently from the mean\n",
    "    dispersion_factor = pm.Exponential('dispersion_factor', lam=1.0)\n",
    "     # The Negative Binomial distribution models the observed byte counts, now influenced by both repository and language effects\n",
    "    language_count = pm.NegativeBinomial('language_count', mu=mu_repo_lang, alpha=dispersion_factor, observed=byte_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb90ed6-45c6-4168-b09a-62dc09530702",
   "metadata": {},
   "source": [
    "Here’s the graphical representation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f96ae5-f91b-4f08-b491-38a1e50d70d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'language_usage_model' is your PyMC model\n",
    "model_graph = pm.model_to_graphviz(language_usage_model)\n",
    "\n",
    "# Customize the graph - Example changes\n",
    "model_graph.attr(label='Hierarchical Language Usage Model\\nBayesian Model Visualization')\n",
    "model_graph.attr(fontsize='20', color='blue', fontname=\"Helvetica\")\n",
    "\n",
    "# Node and Edge customizations\n",
    "with model_graph.subgraph() as s:\n",
    "    s.attr(rank='same')\n",
    "    # Customize nodes\n",
    "    s.node_attr.update(color='lightblue2', style='filled', fontname=\"Helvetica\")\n",
    "    # Customize edges\n",
    "    s.edge_attr.update(color='gray', arrowsize='0.5')\n",
    "\n",
    "# Render the graph \n",
    "model_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b207c9-e05c-43cb-9114-c4b3e660d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fiting the model with MCMC\n",
    "with language_usage_model:\n",
    "    trace = pm.sample(1000, tune=500, return_inferencedata=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549433a2-7215-43e3-90f7-f44294519f4f",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    " - Checking model convergence (e.g., trace plots, R-hat statistics)\n",
    "Posterior predictive checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d3dd7-04e7-4261-aeab-4cd4592cc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the posterior distributions\n",
    "summary = az.summary(trace)\n",
    "\n",
    "# Trace plots for parameter exploration\n",
    "az.plot_trace(trace)\n",
    "\n",
    "# Model diagnostics (e.g., effective sample size, R-hat)\n",
    "diagnostics = az.rhat(trace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d0d40-f642-4632-875b-4a9395e2491c",
   "metadata": {},
   "source": [
    "##  Results and Interpretation\n",
    "- Extracting and summarizing the posterior distributions of model parameters\n",
    "- Identifying significant factors and their impacts on language usage risk metrics\n",
    "- Ridge plots for visualizing the distribution of language usage across projects and repositories, highlighting potential outliers or risks\n",
    "- Additional plots for deeper insights (e.g., comparison of language usage trends across different organizational levels)\n",
    "\n",
    "#### Key Considerations:\n",
    "\n",
    "- **Credible Intervals and Outliers**: Narrow credible intervals suggest high parameter estimate certainty, while wide intervals indicate areas needing further investigation. Outlier detection can pinpoint innovative areas or non-standard practices for strategic exploration.\n",
    "\n",
    "- **Decision-Making**: Insights from the HBM should inform strategic decisions regarding technology adoption, project management, and training to align coding practices with organizational goals, enhancing project continuity and adaptability.\n",
    "\n",
    "This combined understanding of HBM principles, model structure, and practical implications equips readers with the knowledge to interpret complex data analyses meaningfully, driving informed decision-making within the organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a884ea9-56cf-4070-876a-7f4ae955f318",
   "metadata": {},
   "source": [
    "## Streamlit app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
