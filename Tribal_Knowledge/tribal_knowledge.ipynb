{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d3bdff101d217",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Modeling to assess tribal knowledge\n",
    "\n",
    "In this analysis, we aim to develop a methodology and data-driven metric to identify potential technological risks within an organization's coding practices. We will explore the utilisation of programming languages across various projects and repositories, mirroring those found in code repositories such as GitHub. Our approach employs Hierarchical Bayesian Modelling (HBM) for multi-level data analysis, with a keen focus on the effects of programming languages within individual repositories.\n",
    "\n",
    "While our primary interest lies in understanding language effects at the repository level, we also place significant emphasis on the aggregate usage of languages organisation-wide. This dual focus allows us to capture not only repository-specific variances but also overarching language trends, thereby providing a nuanced \"risk\" metric for Enterprise Architecture. Such a metric enables organisations to identify potential knowledge silos, thereby informing strategic decisions aimed at enhancing project continuity and organisational adaptability.\n",
    "\n",
    "By analysing language usage across different organisational strata and integrating measures of uncertainty, HBM aims to expose pockets of siloed tribal knowledge. While initially focusing on language use, this methodology can be expanded to include other metrics, such as the number of commits, time since the last commit, total commits, and more. This approach is crucial for unveiling hidden risks within the architectural framework by uncovering potential vulnerabilities. It facilitates a comparison of language usage at the repository level against broader organisational patterns, offering insights into scenarios where a specific programming language may appear marginal in isolation but emerges as a significant risk in the broader organisational context due to limited expertise or exposure.\n",
    "\n",
    "Consider, for example, a scenario where a single repository predominantly uses Lua, a language not widely adopted in broader enterprise contexts. Our Hierarchical Bayesian Modelling evaluates the risk by meticulously examining Lua's application within the repository, its relevance to the overarching project, and its prevalence within the organisation. This comprehensive assessment is pivotal in ascertaining whether Lua's usage aligns with the enterprise's technological trajectory and knowledge base, thereby guiding strategic architectural decisions and reinforcing organisational resilience in the face of technological evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee99bdbedd31ef3",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "To start building a hierarchical Bayesian model using PyMC3 based on your JSON data, you'll first need to parse the data to extract the relevant information for modelling. This involves aggregating language usage across repositories and projects. After that, we define a hierarchical model that captures the variability within repositories and commonalities across projects.\n",
    "\n",
    "* Data Preparation: Aggregate the language bytes for each language across all repositories and projects.\n",
    "* Model Definition: Define a hierarchical model in Pymc, using project-level priors influencing repository-level distributions.\n",
    "* Inference: Use MCMC provided by pymc to sample from the posterior distribution.\n",
    "* Analysis: Analyze the posterior distributions to identify languages with usage outside the credible regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64929205d7d6c912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:12.460170Z",
     "start_time": "2024-02-27T04:55:12.456226Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (pytensor.configdefaults): g++ not detected!  PyTensor will be unable to compile C-implementations and will default to Python. Performance may be severely degraded. To remove this warning, set PyTensor flags cxx to an empty string.\n",
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import graphviz\n",
    "import arviz as az\n",
    "import pprint \n",
    "from ridgeplot import ridgeplot\n",
    "import matplotlib as plt\n",
    "import warnings\n",
    "\n",
    "from utils import load_data, json2pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0862-2954-49f3-a206-e69b84cddea7",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "The first step is to run the `generate_dummy_data.py` file to make sure we have data to play around with, the generated dummy data is similar to what you might pull from GitHub's REST API for repository languages https://docs.github.com/en/rest/repos/repos?apiVersion=2022-11-28#list-repository-languages\n",
    "\n",
    "```GitHub CLI api\n",
    "https://cli.github.com/manual/gh_api\n",
    "\n",
    "gh api \\\n",
    "  -H \"Accept: application/vnd.github+json\" \\\n",
    "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
    "  /repos/OWNER/REPO/languages\n",
    "\n",
    "Example Response:\n",
    "{\n",
    "  \"C\": 78769,\n",
    "  \"Python\": 7769\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "From here, we can load and transform the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bc6912-e1b0-4ff3-bd41-da2c962a92af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:16.064859Z",
     "start_time": "2024-02-27T04:55:16.057668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Project_1': {'Repo_1': {'C#': 1373390,\n",
      "                          'Groovy': 3208,\n",
      "                          'Java': 3208233,\n",
      "                          'Kotlin': 1241510,\n",
      "                          'Lua': 24122,\n",
      "                          'Rust': 475136,\n",
      "                          'SQL': 903102},\n",
      "               'Repo_2': {'Bash': 129152,\n",
      "                          'Elixir': 74906,\n",
      "                          'Go': 127016,\n",
      "                          'JavaScript': 97343,\n",
      "                          'Kotlin': 201302,\n",
      "                          'Python': 527544,\n",
      "                          'Rails': 18664,\n",
      "                          'TypeScript': 53918},\n",
      "               'Repo_3': {'Rust': 755603, 'Scala': 7693879},\n",
      "               'Repo_4': {'Python': 1432726},\n",
      "               'Repo_5': {'C++': 574406, 'PHP': 673745, 'Python': 884138},\n",
      "               'Repo_6': {'Bash': 2029433,\n",
      "                          'JavaScript': 2675468,\n",
      "                          'Swift': 153707},\n",
      "               'Repo_7': {'C++': 2186775,\n",
      "                          'Elixir': 4,\n",
      "                          'Fortran': 98959,\n",
      "                          'JavaScript': 1538824,\n",
      "                          'Kotlin': 327861,\n",
      "                          'Lua': 13475,\n",
      "                          'MATLAB': 132873,\n",
      "                          'Objective-C': 51924,\n",
      "                          'PowerShell': 831597,\n",
      "                          'Rails': 43935,\n",
      "                          'Ruby': 934377,\n",
      "                          'SQL': 1124922,\n",
      "                          'Swift': 125631,\n",
      "                          'TypeScript': 666786},\n",
      "               'Repo_8': {'C#': 7623361}}}\n"
     ]
    }
   ],
   "source": [
    "df_json = load_data(\"data/dummy_language_data.json\")\n",
    "# Prety print some Projects and Repos randomly to visualise the data\n",
    "NUM_PROJECTS = 1\n",
    "first_N_projects = {k: df_json[k] for k in list(df_json)[:NUM_PROJECTS]}\n",
    "pp = pprint.PrettyPrinter(depth=3)\n",
    "pp.pprint(first_N_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479e6ad-ad19-4f50-bc99-495b84ea55e2",
   "metadata": {},
   "source": [
    "Let's flip this into a normal dataset we are used to, and and a new variable to log transform the byte count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a57fbc-6bc4-4c72-bb1f-858e38ba2b57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T04:55:19.331032Z",
     "start_time": "2024-02-27T04:55:19.315700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of projects: 50\n",
      "Total number of unique repository IDs: 664\n",
      "Total number of unique language-repository combinations: 2853\n"
     ]
    }
   ],
   "source": [
    "df=json2pandas(df_json)\n",
    "\n",
    "# Calculate the logarithm of the ByteCount column for later use in the model\n",
    "df['logByteCount'] = np.log(df['ByteCount'])\n",
    "\n",
    "# Create a new column 'Unique_Repo' by concatenating 'Project' and 'Repository' columns\n",
    "df['Unique_Repo'] = df['Project'] + \"_\" + df['Repository']\n",
    "\n",
    "# Columns to encode into categorical codes\n",
    "columns_to_encode = ['Project', 'Language', 'Repository', 'Unique_Repo']\n",
    "\n",
    "# Encode each column to categorical codes\n",
    "for column_name in columns_to_encode:\n",
    "    df[f'{column_name}_codes'] = df[column_name].astype('category').cat.codes\n",
    "\n",
    "# Create a compound key for each language within each repository\n",
    "df['Repo_Lang_Key'] = df['Unique_Repo_codes'].astype(str) + '_' + df['Language_codes'].astype(str)\n",
    "\n",
    "# Encode the compound key into unique categorical codes\n",
    "df['Repo_Lang_Key_codes'] = df['Repo_Lang_Key'].astype('category').cat.codes\n",
    "\n",
    "# Display the total number of unique projects and unique repository IDs for verification\n",
    "print(\"Total number of projects:\", df['Project'].nunique())\n",
    "print(\"Total number of unique repository IDs:\", df['Unique_Repo_codes'].nunique())\n",
    "print(\"Total number of unique language-repository combinations:\", df['Repo_Lang_Key_codes'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a0ed198-d69f-4434-94c7-541578877e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Project</th>\n",
       "      <th>Repository</th>\n",
       "      <th>Language</th>\n",
       "      <th>ByteCount</th>\n",
       "      <th>logByteCount</th>\n",
       "      <th>Unique_Repo</th>\n",
       "      <th>Project_codes</th>\n",
       "      <th>Language_codes</th>\n",
       "      <th>Repository_codes</th>\n",
       "      <th>Unique_Repo_codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>Java</td>\n",
       "      <td>3208233</td>\n",
       "      <td>14.981231</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>SQL</td>\n",
       "      <td>903102</td>\n",
       "      <td>13.713591</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>Groovy</td>\n",
       "      <td>3208</td>\n",
       "      <td>8.073403</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>Rust</td>\n",
       "      <td>475136</td>\n",
       "      <td>13.071356</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>C#</td>\n",
       "      <td>1373390</td>\n",
       "      <td>14.132793</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>Lua</td>\n",
       "      <td>24122</td>\n",
       "      <td>10.090880</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_1</td>\n",
       "      <td>Kotlin</td>\n",
       "      <td>1241510</td>\n",
       "      <td>14.031839</td>\n",
       "      <td>Project_1_Repo_1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_2</td>\n",
       "      <td>Bash</td>\n",
       "      <td>129152</td>\n",
       "      <td>11.768745</td>\n",
       "      <td>Project_1_Repo_2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_2</td>\n",
       "      <td>Kotlin</td>\n",
       "      <td>201302</td>\n",
       "      <td>12.212562</td>\n",
       "      <td>Project_1_Repo_2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Project_1</td>\n",
       "      <td>Repo_2</td>\n",
       "      <td>Python</td>\n",
       "      <td>527544</td>\n",
       "      <td>13.175988</td>\n",
       "      <td>Project_1_Repo_2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Project Repository Language  ByteCount  logByteCount       Unique_Repo  \\\n",
       "0  Project_1     Repo_1     Java    3208233     14.981231  Project_1_Repo_1   \n",
       "1  Project_1     Repo_1      SQL     903102     13.713591  Project_1_Repo_1   \n",
       "2  Project_1     Repo_1   Groovy       3208      8.073403  Project_1_Repo_1   \n",
       "3  Project_1     Repo_1     Rust     475136     13.071356  Project_1_Repo_1   \n",
       "4  Project_1     Repo_1       C#    1373390     14.132793  Project_1_Repo_1   \n",
       "5  Project_1     Repo_1      Lua      24122     10.090880  Project_1_Repo_1   \n",
       "6  Project_1     Repo_1   Kotlin    1241510     14.031839  Project_1_Repo_1   \n",
       "7  Project_1     Repo_2     Bash     129152     11.768745  Project_1_Repo_2   \n",
       "8  Project_1     Repo_2   Kotlin     201302     12.212562  Project_1_Repo_2   \n",
       "9  Project_1     Repo_2   Python     527544     13.175988  Project_1_Repo_2   \n",
       "\n",
       "   Project_codes  Language_codes  Repository_codes  Unique_Repo_codes  \n",
       "0              0              16                 0                100  \n",
       "1              0              32                 0                100  \n",
       "2              0              13                 0                100  \n",
       "3              0              31                 0                100  \n",
       "4              0               2                 0                100  \n",
       "5              0              20                 0                100  \n",
       "6              0              19                 0                100  \n",
       "7              0               1                11                101  \n",
       "8              0              19                11                101  \n",
       "9              0              27                11                101  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9705ad-dbf3-49da-ba26-922f83137c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Language' and create a list of log byte counts for each language\n",
    "grouped = df.groupby('Language')['logByteCount'].apply(list).to_dict()\n",
    "\n",
    "# Prepare samples for ridge plot\n",
    "samples = [np.array(grouped[language]) for language in grouped]\n",
    "fig = ridgeplot(samples=samples, labels=list(grouped.keys()), colorscale=\"viridis\", colormode=\"row-index\",)\n",
    "fig.update_layout(height=1250, width=800)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fe316-0f6a-4b26-8623-8f30ecb6b171",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hierarchical Model Specification\n",
    "\n",
    "This section introduces Hierarchical Bayesian Modelling (HBM) principles and their application in structuring complex, multi-level datasets, such as those encountered in evaluating technological risks within coding languages.\n",
    "\n",
    "#### Introduction to Hierarchical Bayesian Modelling\n",
    "Hierarchical Bayesian Modelling is a statistical framework that facilitates data analysis across different levels of hierarchy by integrating the variability within individual units, such as repositories, and observing commonalities that may emerge. At the core of Bayesian inference is Bayes' theorem, which updates the probability for a hypothesis by introducing new evidence. A fundamental concept in HBM is exchangeability, suggesting that data points are probabilistically interchangeable. This characteristic makes it particularly suited for modelling datasets without a natural ordering but considered identically distributed given some parameters.\n",
    "\n",
    "Our analysis delves into programming language usage within repositories, examining the ByteCount—the code volume per language in each repository. We adopt distributions like the Negative Binomial to model count data, framing our approach as a Hierarchical Negative Binomial Model. This model explicitly addresses the intricacies of language usage within repositories, adding a novel layer by considering the impact of language diversity per repository on language prevalence.\n",
    "\n",
    "- **Repository-Level Language likelihood:** The effects of specific programming languages within repositories are central to our analysis. We model these effects to capture the observed code volume across languages, incorporating a likelihood function that accounts for data variability and dispersion:\n",
    "\n",
    "$$ P(Language_{ij} | \\theta_{ij}) \\sim NegativeBinomial(\\mu_{ij}, \\alpha)$$\n",
    "\n",
    "  Here, $\\\\theta_{ij}$ represents the expected byte count for language usage, where $i$ denotes the repository and $j$ is the language. $\\\\mu_{ij}$ is the mean parameter, and $\\\\alpha$ is the dispersion parameter of the Negative Binomial distribution, modelling the count data's overdispersion within each repository for each language.\n",
    "\n",
    "- **Language Count Priors:**  Reflecting our initial beliefs about code distribution across programming languages within repositories, our model establishes language count priors:\n",
    "\n",
    "$$ \\mu_i \\sim LogNormal(\\log(\\mu), \\sigma^2) $$\n",
    "\n",
    "  This LogNormal distribution for $\\\\mu_i$ allows us to model the mean language counts on a log scale, accommodating the wide range of byte counts observed across languages.\n",
    "\n",
    "- **Language Diversity and Interaction Effects:** A novel aspect of our model focuses on the total number of programming languages present in each repository and their influence on the prevalence of specific languages. This analysis allows us to explore whether repositories with a greater variety of languages exhibit distinct patterns in language usage compared to those with fewer languages.\n",
    "    $$log\\_mu = \\log(diversity\\_effect_{i} + language\\_effect_{j})$$\n",
    "where\n",
    "    - **Diversity Effect:** The effect of language diversity within repositories is modelled quantitatively to predict the prevalence of specific languages. We hypothesise that a higher diversity of languages might correlate with unique language patterns, which is captured by the following equation:\n",
    "    $$ diversity\\_effect \\sim Normal(0, \\sigma_{diversity}) $$\n",
    "      \n",
    "    - **Inferred Interaction Effects:** Our model infers interactions between languages based on their co-occurrence within repositories to capture the nuanced dynamics of language co-occurrence. This approach allows for identifying emergent patterns not explicitly defined by predefined affinities but discovered through data analysis. The inferred interaction effects can be conceptually represented as:\n",
    "    $$ interaction\\_effect \\sim Normal(0, \\sigma_{interaction}) $$\n",
    "\n",
    "The inclusion of language diversity and the potential for inferred interaction effects between languages are modelled implicitly within the log_mu formulation. This allows us to explore how diverse language environments within repositories might influence the prevalence of specific languages, either amplifying or mitigating their usage based on the repository's unique language composition.    \n",
    "   \n",
    "  \n",
    "- **Organisational-Level Hyperpriors**\n",
    "\n",
    "To enhance our repository-level analysis, we incorporate organisational-level hyperpriors. These hyperpriors play a crucial role in deciphering broad patterns of programming language utilisation across the entire organisational spectrum, covering all projects and repositories. By introducing hyperpriors for the mean and dispersion of language usage organisation-wide, we delve into the depths of the organisational programming language landscape, unveiling insights into commonalities and variances that permeate the organisation.\n",
    "\n",
    "- **Modeling Mean Language Usage Across the Organisation with a Beta Distribution**\n",
    "\n",
    "  The mean language usage across the organisation is modelled using a Beta distribution. This choice reflects our understanding that rates of programming language usage can be effectively represented as proportions within the [0,1] range. The flexibility of the Beta distribution makes it ideally suited for capturing the varied prevalence of programming languages across repositories. The mean language usage is defined by:\n",
    "\n",
    "  $$\\mu_i \\sim Beta(a_{\\mu}, b_{\\mu})$$\n",
    "\n",
    "  where $a_{\\mu}$ and $b_{\\mu}$ are the shape parameters of the Beta distribution. These parameters guide our beliefs about the distribution of mean language usage across the organisation, providing a foundation for analysing language trends.\n",
    "\n",
    "- **Dispersion in Language Usage**\n",
    "\n",
    "  The dispersion in language usage, reflecting the variability or concentration of language distribution organisation-wide, is modelled using a Gamma distribution. This approach accommodates both concentrated and dispersed patterns of language usage, capturing the essence of variability in language patterns across the organisation:\n",
    "\n",
    "  $$\\kappa_i \\sim Gamma(a_{\\kappa}, b_{\\kappa})$$\n",
    "\n",
    "  The parameters $a_{\\kappa}$ and $b_{\\kappa}$ are specifically chosen to reflect the observed spread in language utilisation, ensuring the model accurately represents the range of distributions from concentrated to broadly dispersed. Integrating this dispersion parameter into our hierarchical Bayesian model enhances our understanding of the dynamics governing programming language prevalence and variability within the organisational context.\n",
    "\n",
    "#### An English Explanation of Our Model\n",
    "\n",
    "- **Negative Binomial Distribution for Count Data:** The model employs the Negative Binomial distribution to accurately reflect the variability in the number of bytes of code (ByteCount) across different programming languages in each repository. This choice addresses the common overdispersion in code volume data, allowing for a more realistic representation of code distribution patterns.\n",
    "\n",
    "- **Model Components:**\n",
    "\n",
    "  - **Language Likelihood at the Repository Level:** This component assesses the likelihood of encountering specific volumes of code in various languages within each repository. It accounts for both the average volume of code (mean) and the variability (variance) in these volumes, acknowledging the significant differences that can exist not only between languages but also within the same language across different repositories.\n",
    "\n",
    "  - **Language Count Priors:** The model establishes baseline assumptions regarding code distribution across programming languages within repositories, employing a LogNormal distribution. This choice reflects the expectation that code volumes can span a broad range, with some languages potentially having significantly more significant amounts of code. The LogNormal distribution is suited for capturing this wide range, especially on a logarithmic scale, where the distribution of code volumes is presumed to be skewed.\n",
    "\n",
    "  - **Language Diversity and Interaction Effects:** A key innovation of the model is its attention to the diversity of programming languages present within a repository and the potential interactions between these languages. It posits that repositories featuring a more comprehensive array of languages may demonstrate distinct language usage patterns compared to those with fewer languages. The model captures these dynamics through a composite effect ($log_{mu}$), which integrates the influence of language diversity and the interactions between languages that co-occur within repositories.\n",
    "\n",
    "  - **Organizational-Level Hyperpriors:** Extending its analysis beyond the scope of individual repositories, the model integrates organisational-level hyperpriors to discern overarching patterns of programming language use throughout the entire organisation. These hyperpriors, informed by distributions such as the Beta for mean language usage and the Gamma for dispersion in language usage, enable the model to capture broad trends and variabilities in language distribution across the organisation. This holistic view facilitates the identification of widespread language usage patterns, potential areas for technology adoption, and the detection of language-specific knowledge silos.\n",
    "\n",
    "- **Objective of the Model:** The primary aim of this model is to derive actionable insights into the technological ecosystem of an organisation by meticulously analysing programming language usage at both micro (repository) and macro (organisational) levels. Through iTheeks to, uncover areas ripe for strategic intervention, risk management, and resource optimisation. Through its hierarchical Bayesian framework of dominant languages and examining the nuances of language diversity and interaction, the model offers a comprehensive understanding of the organisation's coding practices to foster innovation, enhance efficiency, and guide technological strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666c696-acdd-48d5-ba48-dfd4480b6f0d",
   "metadata": {},
   "source": [
    "\n",
    "## Transitioning to Posterior Distributions\n",
    "\n",
    "### From Theory to Practice: The Role of Posterior Distributions\n",
    "\n",
    "With our model parameters defined and their priors set, the next step in Bayesian analysis is to update these beliefs with observed data. This is where the posterior distribution comes into play.\n",
    "\n",
    "\n",
    "#### What is the posterior?\n",
    "\n",
    "In the hierarchical Bayesian modelling (HBM) context, the posterior distribution is the updated belief about our model's parameters after considering the observed data. It combines our prior beliefs (the priors) and the evidence from the data (the likelihood). Mathematically, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(\\theta | data) \\propto P(data | \\theta) \\times P(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ P(\\theta | data) $\\) is the posterior distribution of the parameters \\($ \\theta $\\).\n",
    "- \\($ P(data | \\theta) $\\) is the likelihood of the data given the parameters.\n",
    "- \\($ P(\\theta) $\\) is the prior distribution of the parameters.\n",
    "\n",
    "After observing the data, the posterior distribution provides a range of likely values for the parameters, which is crucial for making informed decisions.\n",
    "\n",
    "\n",
    "### Practical Implications of Posterior Analysis in Hierarchical Bayesian Modelling\n",
    "### Informing Strategic Decision-Making\n",
    "\n",
    "The power of posterior analysis extends beyond diagnostics; it informs strategic resource allocation and risk management:\n",
    "\n",
    "- **Credible Intervals**: The precision of parameter estimates, reflected in the credible intervals of the posterior distribution, directs our focus to areas where additional data collection or deeper investigation may be warranted.\n",
    "\n",
    "- **Outlier Detection**: Spotting outliers within posterior distributions alerts us to unconventional language usage patterns. These could represent areas of innovation warranting further exploration or potential risks if the languages in question lack broad support.\n",
    "\n",
    "- **Strategic Resource Allocation**: Insights gained from posterior distributions enable informed decisions on resource allocation—be it for targeted training programmes, strategic hiring to build expertise in underutilised languages, or investment in technology stacks that promise to align with and propel the organisation's strategic objectives.\n",
    "\n",
    "Interpreting the posterior distributions derived from our hierarchical model does more than just enhance our understanding of language usage; it equips us to forecast, plan, and foster a coding environment that is both efficient and resilient to future challenges.\n",
    "\n",
    "### Uncovering Knowledge Silos\n",
    "\n",
    "The posterior distributions for language usage within repositories serve as a diagnostic tool, revealing languages that are disproportionately relied upon. Anomalies in these distributions may signal the existence of knowledge silos, suggesting areas where diversification and training could be beneficial. By identifying these silos, we can proactively address potential bottlenecks in knowledge transfer and code maintenance.\n",
    "\n",
    "### Assessing Project-Level Variability\n",
    "\n",
    "The consistency of coding practices across repositories within projects is characterised by project-level hyperparameters. When significant variability is observed in these posterior distributions, it may reflect fragmented coding practices that could undermine team collaboration and project efficiency. This insight drives us to review and possibly revise coding standards, ensuring that practices are aligned and conducive to project success.\n",
    "\n",
    "### Evaluating Organisational Coding Norms\n",
    "\n",
    "At the highest organisational level, posterior distributions offer a macro perspective of coding culture and norms. Deviations in these distributions can reveal organisational preferences or aversions towards specific languages. Understanding these trends is critical for shaping future strategies in technology adoption, capability development, and training initiatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280421-54eb-4c68-8cb1-f28ebb96f6cb",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- Implementing the HBM using PyMC3\n",
    "- Defining the model in PyMC3\n",
    "- Setting up the priors for each level of the hierarchy\n",
    "- Incorporating the data into the model\n",
    "- Model fitting (e.g., using MCMC methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc360bd-a6f5-49ab-a82b-9a8379efa9da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-27T06:23:43.619461Z",
     "start_time": "2024-02-27T06:23:43.590571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_projects: 50, n_repositories: 664, n_languages: 37\n",
      "n_language_repo_combinations: 2853\n",
      "language_repo_idx shape: (2853,), byte_count shape: (2853,)\n"
     ]
    }
   ],
   "source": [
    "n_projects = df['Project_codes'].nunique()\n",
    "# Update to use the 'Unique_Repo_codes' for repository count since it's already unique across projects\n",
    "n_repositories = df['Unique_Repo_codes'].nunique()\n",
    "# Continue using 'Language_codes' for language count as it remains valid\n",
    "n_languages = df['Language_codes'].nunique()\n",
    "\n",
    "# Update to calculate the number of unique language-repository combinations instead\n",
    "n_language_repo_combinations = df['Repo_Lang_Key_codes'].nunique()\n",
    "\n",
    "# Updated indexing to use 'Repo_Lang_Key_codes' for a unique identifier of language within repositories\n",
    "language_repo_idx = df['Repo_Lang_Key_codes'].values\n",
    "\n",
    "# Byte count remains the same as it's tied to the unique language-repository combination\n",
    "byte_count = df['ByteCount'].values\n",
    "\n",
    "print(f'n_projects: {n_projects}, n_repositories: {n_repositories}, n_languages: {n_languages}')\n",
    "print(f'n_language_repo_combinations: {n_language_repo_combinations}')\n",
    "print(f'language_repo_idx shape: {language_repo_idx.shape}, byte_count shape: {byte_count.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b882230e-57d8-4bf4-8cdd-4ec72954db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with pm.Model() as language_usage_model:\n",
    "    # Hyperpriors for mean and dispersion across the organization\n",
    "    a_mu_mean = pm.Gamma('a_mu_mean', alpha=1.0, beta=1.0)\n",
    "    b_mu_dispersion = pm.Gamma('b_mu_dispersion', alpha=1.0, beta=1.0)\n",
    "    \n",
    "    # Hyperpriors for the diversity effect\n",
    "    alpha_diversity = pm.Gamma('alpha_diversity', alpha=1.0, beta=1.0)\n",
    "    beta_diversity = pm.Gamma('beta_diversity', alpha=1.0, beta=1.0)\n",
    "    \n",
    "    # Language-specific priors for mean effects\n",
    "    language_specific_means = pm.Normal('language_specific_means', \n",
    "                                        mu=a_mu_mean, \n",
    "                                        sigma=10, \n",
    "                                        shape=n_languages)\n",
    "    \n",
    "    # Language-specific standard deviations\n",
    "    language_specific_sds = pm.HalfNormal('language_specific_sds', \n",
    "                                          sigma=pm.math.sqrt(1 / b_mu_dispersion), \n",
    "                                          shape=n_languages)\n",
    "    \n",
    "    # Affinity of each language to its repository\n",
    "    language_repo_affinity = pm.Beta('language_repo_affinity', \n",
    "                                     alpha=1, beta=1, \n",
    "                                     shape=n_language_repo_combinations)\n",
    "    \n",
    "    # Assuming `uniform_language_effect` was meant to be a baseline effect across languages,\n",
    "    # and `repository_idx_for_combinations` maps language-repository combinations to repositories.\n",
    "    # This requires clarification or adjustment based on your data structure.\n",
    "\n",
    "    # Adjusted language effect considering repository-level baseline and language-repository affinity\n",
    "    adjusted_language_effect = pm.Normal('adjusted_language_effect', \n",
    "                                         mu=language_specific_means[language_idx],  # Correct indexing\n",
    "                                         sigma=language_specific_sds[language_idx],  # Correct indexing\n",
    "                                         shape=n_language_repo_combinations) * language_repo_affinity\n",
    "    \n",
    "    # Diversity effect modeled for each language-repository combination\n",
    "    diversity_effect = pm.Gamma('diversity_effect', \n",
    "                                alpha=alpha_diversity, \n",
    "                                beta=beta_diversity, \n",
    "                                shape=n_language_repo_combinations)\n",
    "    \n",
    "    # Expected byte counts, combining adjusted language effect and diversity effect\n",
    "    log_mu = pm.math.log(adjusted_language_effect + diversity_effect)\n",
    "    \n",
    "    # Dispersion parameter for the Negative Binomial distribution\n",
    "    alpha_parameter = pm.Exponential('alpha_parameter', 1.0, shape=n_language_repo_combinations)\n",
    "    \n",
    "    # Negative Binomial distribution for observed byte counts\n",
    "    language_count_obs = pm.NegativeBinomial('language_count_obs', \n",
    "                                             mu=np.exp(log_mu), \n",
    "                                             alpha=alpha_parameter, \n",
    "                                             observed=byte_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb90ed6-45c6-4168-b09a-62dc09530702",
   "metadata": {},
   "source": [
    "Here’s the graphical representation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02f96ae5-f91b-4f08-b491-38a1e50d70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"946pt\" height=\"536pt\"\n",
       " viewBox=\"0.00 0.00 946.47 536.04\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 532.04)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-532.04 942.47,-532.04 942.47,4 -4,4\"/>\n",
       "<text text-anchor=\"middle\" x=\"469.23\" y=\"-33\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Hierarchical Language Usage Model</text>\n",
       "<text text-anchor=\"middle\" x=\"469.23\" y=\"-9\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Bayesian Model Visualization</text>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster2853</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.47,-64C152.47,-64 918.47,-64 918.47,-64 924.47,-64 930.47,-70 930.47,-76 930.47,-76 930.47,-284.02 930.47,-284.02 930.47,-290.02 924.47,-296.02 918.47,-296.02 918.47,-296.02 152.47,-296.02 152.47,-296.02 146.47,-296.02 140.47,-290.02 140.47,-284.02 140.47,-284.02 140.47,-76 140.47,-76 140.47,-70 146.47,-64 152.47,-64\"/>\n",
       "<text text-anchor=\"middle\" x=\"908.97\" y=\"-70.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">2853</text>\n",
       "</g>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster37</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M313.47,-304.02C313.47,-304.02 747.47,-304.02 747.47,-304.02 753.47,-304.02 759.47,-310.02 759.47,-316.02 759.47,-316.02 759.47,-409.91 759.47,-409.91 759.47,-415.91 753.47,-421.91 747.47,-421.91 747.47,-421.91 313.47,-421.91 313.47,-421.91 307.47,-421.91 301.47,-415.91 301.47,-409.91 301.47,-409.91 301.47,-316.02 301.47,-316.02 301.47,-310.02 307.47,-304.02 313.47,-304.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"744.72\" y=\"-310.47\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">37</text>\n",
       "</g>\n",
       "<!-- beta_diversity -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>beta_diversity</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.47\" cy=\"-374.84\" rx=\"66.47\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.47\" y=\"-385.16\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">beta_diversity</text>\n",
       "<text text-anchor=\"middle\" x=\"66.47\" y=\"-369.41\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"66.47\" y=\"-353.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gamma</text>\n",
       "</g>\n",
       "<!-- diversity_effect -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>diversity_effect</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"220.47\" cy=\"-248.95\" rx=\"72.3\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"220.47\" y=\"-259.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">diversity_effect</text>\n",
       "<text text-anchor=\"middle\" x=\"220.47\" y=\"-243.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"220.47\" y=\"-227.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gamma</text>\n",
       "</g>\n",
       "<!-- beta_diversity&#45;&gt;diversity_effect -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>beta_diversity&#45;&gt;diversity_effect</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M99.86,-340.75C112.52,-328.74 127.3,-315.36 141.47,-304.02 149.1,-297.91 157.41,-291.75 165.66,-285.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"167.49,-288.88 173.68,-280.28 163.48,-283.14 167.49,-288.88\"/>\n",
       "</g>\n",
       "<!-- alpha_diversity -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>alpha_diversity</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"221.47\" cy=\"-374.84\" rx=\"70.71\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"221.47\" y=\"-385.16\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">alpha_diversity</text>\n",
       "<text text-anchor=\"middle\" x=\"221.47\" y=\"-369.41\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"221.47\" y=\"-353.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gamma</text>\n",
       "</g>\n",
       "<!-- alpha_diversity&#45;&gt;diversity_effect -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>alpha_diversity&#45;&gt;diversity_effect</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.16,-335.32C221.07,-324.02 220.96,-311.49 220.87,-299.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"224.37,-299.93 220.79,-289.96 217.37,-299.99 224.37,-299.93\"/>\n",
       "</g>\n",
       "<!-- a_mu_mean -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>a_mu_mean</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"421.47\" cy=\"-488.97\" rx=\"59.04\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-499.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">a_mu_mean</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-483.55\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-467.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gamma</text>\n",
       "</g>\n",
       "<!-- language_specific_means -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>language_specific_means</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"421.47\" cy=\"-374.84\" rx=\"111.55\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-385.16\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">language_specific_means</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-369.41\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-353.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normal</text>\n",
       "</g>\n",
       "<!-- a_mu_mean&#45;&gt;language_specific_means -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a_mu_mean&#45;&gt;language_specific_means</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.47,-449.7C421.47,-441.93 421.47,-433.66 421.47,-425.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"424.97,-425.84 421.47,-415.84 417.97,-425.84 424.97,-425.84\"/>\n",
       "</g>\n",
       "<!-- b_mu_dispersion -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>b_mu_dispersion</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"651.47\" cy=\"-488.97\" rx=\"80.26\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-499.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b_mu_dispersion</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-483.55\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-467.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gamma</text>\n",
       "</g>\n",
       "<!-- language_specific_sds -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>language_specific_sds</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"651.47\" cy=\"-374.84\" rx=\"100.41\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-385.16\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">language_specific_sds</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-369.41\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-353.66\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">HalfNormal</text>\n",
       "</g>\n",
       "<!-- b_mu_dispersion&#45;&gt;language_specific_sds -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>b_mu_dispersion&#45;&gt;language_specific_sds</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M651.47,-449.7C651.47,-441.93 651.47,-433.66 651.47,-425.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"654.97,-425.84 651.47,-415.84 647.97,-425.84 654.97,-425.84\"/>\n",
       "</g>\n",
       "<!-- adjusted_language_effect -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>adjusted_language_effect</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"421.47\" cy=\"-248.95\" rx=\"111.02\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-259.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">adjusted_language_effect</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-243.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"421.47\" y=\"-227.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Normal</text>\n",
       "</g>\n",
       "<!-- language_specific_means&#45;&gt;adjusted_language_effect -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>language_specific_means&#45;&gt;adjusted_language_effect</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M421.47,-335.32C421.47,-324.02 421.47,-311.49 421.47,-299.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"424.97,-299.96 421.47,-289.96 417.97,-299.96 424.97,-299.96\"/>\n",
       "</g>\n",
       "<!-- language_specific_sds&#45;&gt;adjusted_language_effect -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>language_specific_sds&#45;&gt;adjusted_language_effect</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M602.49,-340.26C584.04,-328.2 562.64,-314.91 542.47,-304.02 529.72,-297.14 515.93,-290.37 502.37,-284.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"504,-280.98 493.45,-280 501.08,-287.34 504,-280.98\"/>\n",
       "</g>\n",
       "<!-- language_count_obs -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>language_count_obs</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"536.47\" cy=\"-134.82\" rx=\"92.98\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"536.47\" y=\"-145.14\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">language_count_obs</text>\n",
       "<text text-anchor=\"middle\" x=\"536.47\" y=\"-129.39\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"536.47\" y=\"-113.64\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">NegBinomial</text>\n",
       "</g>\n",
       "<!-- alpha_parameter -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>alpha_parameter</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"846.47\" cy=\"-248.95\" rx=\"76.01\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"846.47\" y=\"-259.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">alpha_parameter</text>\n",
       "<text text-anchor=\"middle\" x=\"846.47\" y=\"-243.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"846.47\" y=\"-227.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Exponential</text>\n",
       "</g>\n",
       "<!-- alpha_parameter&#45;&gt;language_count_obs -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>alpha_parameter&#45;&gt;language_count_obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M790.57,-221.94C780.94,-217.74 770.97,-213.56 761.47,-209.89 716.05,-192.29 664.49,-175.12 622.2,-161.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"623.42,-158.47 612.83,-158.8 621.32,-165.14 623.42,-158.47\"/>\n",
       "</g>\n",
       "<!-- language_repo_affinity -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>language_repo_affinity</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"651.47\" cy=\"-248.95\" rx=\"100.94\" ry=\"39.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-259.28\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">language_repo_affinity</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-243.53\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">~</text>\n",
       "<text text-anchor=\"middle\" x=\"651.47\" y=\"-227.78\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Beta</text>\n",
       "</g>\n",
       "<!-- language_repo_affinity&#45;&gt;language_count_obs -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>language_repo_affinity&#45;&gt;language_count_obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M614.77,-212.16C603.97,-201.64 592.05,-190.02 580.84,-179.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"583.51,-176.8 573.91,-172.32 578.62,-181.81 583.51,-176.8\"/>\n",
       "</g>\n",
       "<!-- adjusted_language_effect&#45;&gt;language_count_obs -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>adjusted_language_effect&#45;&gt;language_count_obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M458.49,-211.86C469.27,-201.35 481.15,-189.76 492.32,-178.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"494.5,-181.63 499.22,-172.14 489.62,-176.61 494.5,-181.63\"/>\n",
       "</g>\n",
       "<!-- diversity_effect&#45;&gt;language_count_obs -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>diversity_effect&#45;&gt;language_count_obs</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M273.18,-221.94C282.51,-217.69 292.21,-213.5 301.47,-209.89 349.41,-191.16 404.21,-173.67 448.82,-160.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.58,-163.82 458.18,-157.63 447.6,-157.11 449.58,-163.82\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x155007fa650>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming 'language_usage_model' is your PyMC model\n",
    "model_graph = pm.model_to_graphviz(language_usage_model)\n",
    "\n",
    "# Customize the graph - Example changes\n",
    "model_graph.attr(label='Hierarchical Language Usage Model\\nBayesian Model Visualization')\n",
    "model_graph.attr(fontsize='20', color='blue', fontname=\"Helvetica\")\n",
    "\n",
    "# Node and Edge customizations\n",
    "with model_graph.subgraph() as s:\n",
    "    s.attr(rank='same')\n",
    "    # Customize nodes\n",
    "    s.node_attr.update(color='lightblue2', style='filled', fontname=\"Helvetica\")\n",
    "    # Customize edges\n",
    "    s.edge_attr.update(color='gray', arrowsize='0.5')\n",
    "\n",
    "# Render the graph \n",
    "model_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8b207c9-e05c-43cb-9114-c4b3e660d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VCB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\tensor\\rewriting\\elemwise.py:1029: UserWarning: Loop fusion failed because the resulting node would exceed the kernel argument limit.\n",
      "  warn(\n",
      "C:\\Users\\VCB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pytensor\\tensor\\rewriting\\elemwise.py:1029: UserWarning: Loop fusion failed because the resulting node would exceed the kernel argument limit.\n",
      "  warn(\n",
      "C:\\Users\\VCB\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\nutpie\\compile_pymc.py:400: NumbaWarning: \u001b[1m\u001b[1mCannot cache compiled function \"numba_funcified_fgraph\" as it uses dynamic globals (such as ctypes pointers and large global arrays)\u001b[0m\u001b[0m\n",
      "  return inner(x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/6000 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Sampling failed: All initialization points failed\n\nCaused by:\n    Logp function returned error: Logp function returned error code 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fiting the model - using nutpie for faster sampling\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Im using nutpie here as i dont have a gpu available -> change to take advantage.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m language_usage_model:\n\u001b[1;32m----> 5\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnuts_sampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnutpie\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:689\u001b[0m, in \u001b[0;36msample\u001b[1;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, NUTS):\n\u001b[0;32m    686\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    687\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel can not be sampled with NUTS alone. Your model is probably not continuous.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    688\u001b[0m         )\n\u001b[1;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_external_nuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnuts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget_accept\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitvals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitvals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(step, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    705\u001b[0m     step \u001b[38;5;241m=\u001b[39m CompoundStep(step)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:297\u001b[0m, in \u001b[0;36m_sample_external_nuts\u001b[1;34m(sampler, draws, tune, chains, target_accept, random_seed, initvals, model, progressbar, idata_kwargs, nuts_sampler_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    295\u001b[0m compiled_model \u001b[38;5;241m=\u001b[39m nutpie\u001b[38;5;241m.\u001b[39mcompile_pymc_model(model)\n\u001b[0;32m    296\u001b[0m t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 297\u001b[0m idata \u001b[38;5;241m=\u001b[39m \u001b[43mnutpie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_accept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_get_seeds_per_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnuts_sampler_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m t_sample \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Temporary work-around. Revert once https://github.com/pymc-devs/nutpie/issues/74 is fixed\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# gather observed and constant data as nutpie.sample() has no access to the PyMC model\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tribal-knowledge-tLBhIDXi-py3.11\\Lib\\site-packages\\nutpie\\sample.py:236\u001b[0m, in \u001b[0;36msample\u001b[1;34m(compiled_model, draws, tune, chains, cores, seed, save_warmup, progress_bar, init_mean, return_raw_trace, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     results \u001b[38;5;241m=\u001b[39m sampler\u001b[38;5;241m.\u001b[39mfinalize()\n\u001b[0;32m    238\u001b[0m dims \u001b[38;5;241m=\u001b[39m {name: \u001b[38;5;28mlist\u001b[39m(dim) \u001b[38;5;28;01mfor\u001b[39;00m name, dim \u001b[38;5;129;01min\u001b[39;00m compiled_model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    239\u001b[0m dims[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmass_matrix_inv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconstrained_parameter\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Sampling failed: All initialization points failed\n\nCaused by:\n    Logp function returned error: Logp function returned error code 1"
     ]
    }
   ],
   "source": [
    "# Fiting the model - using nutpie for faster sampling\n",
    "# Im using nutpie here as i dont have a gpu available -> change to take advantage.\n",
    "\n",
    "with language_usage_model:\n",
    "    trace = pm.sample(1000, tune=500, return_inferencedata=True, nuts_sampler=\"nutpie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549433a2-7215-43e3-90f7-f44294519f4f",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    " - Checking model convergence (e.g., trace plots, R-hat statistics)\n",
    "Posterior predictive checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d3dd7-04e7-4261-aeab-4cd4592cc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary stats for the language.\n",
    "print(az.summary(trace, var_names=\"language_effect\"))\n",
    "az.plot_posterior(trace, var_names=[\"language_effect\"])\n",
    "\n",
    "# Plotting\n",
    "axes = az.plot_forest(\n",
    "    trace,\n",
    "    kind='ridgeplot',\n",
    "    var_names=[\"language_effect\"],\n",
    "    figsize=(10, 10),\n",
    "    combined=True,\n",
    "    ridgeplot_overlap=3,\n",
    "    colors=\"cycle\",\n",
    "    ridgeplot_truncate=False,\n",
    "    ridgeplot_quantiles=[0.25, 0.5, 0.75],\n",
    ")\n",
    "\n",
    "# Assuming the first (or only) axis corresponds to the 'language_effect' variable\n",
    "axes[0].set_yticklabels(language_to_code_sorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d8938-24eb-4451-9e63-17810d47ea1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527d0d40-f642-4632-875b-4a9395e2491c",
   "metadata": {},
   "source": [
    "##  Results and Interpretation\n",
    "- Extracting and summarizing the posterior distributions of model parameters\n",
    "- Identifying significant factors and their impacts on language usage risk metrics\n",
    "- Ridge plots for visualizing the distribution of language usage across projects and repositories, highlighting potential outliers or risks\n",
    "- Additional plots for deeper insights (e.g., comparison of language usage trends across different organizational levels)\n",
    "\n",
    "#### Key Considerations:\n",
    "\n",
    "- **Credible Intervals and Outliers**: Narrow credible intervals suggest high parameter estimate certainty, while wide intervals indicate areas needing further investigation. Outlier detection can pinpoint innovative areas or non-standard practices for strategic exploration.\n",
    "\n",
    "- **Decision-Making**: Insights from the HBM should inform strategic decisions regarding technology adoption, project management, and training to align coding practices with organizational goals, enhancing project continuity and adaptability.\n",
    "\n",
    "This combined understanding of HBM principles, model structure, and practical implications equips readers with the knowledge to interpret complex data analyses meaningfully, driving informed decision-making within the organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a884ea9-56cf-4070-876a-7f4ae955f318",
   "metadata": {},
   "source": [
    "## PRedicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114e475-4a16-4e2c-aac6-5b325a6c1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup for a new repository with Lua and a specific byte count\n",
    "\n",
    "#Its a new repo so just make the max + 1 \n",
    "new_repo_index = df['Unique_Repo_codes'].max() + 1\n",
    "# Get language Code\n",
    "lua_language_code = df[df['Language'] == 'Lua']['Language_codes'].unique()[0]\n",
    "print(lua_language_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ea547-0174-41a5-911d-84d7b71a63a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example setup for a new repository with Lua and a specific byte count\n",
    "\n",
    "new_repo_data = {\n",
    "    'repository_idx': np.array([new_repo_index]), \n",
    "    'languages_idx': np.array([lua_language_code]),  # Replace with the encoded index for Lua\n",
    "    'byte_count': np.array([50000000]),  # Byte count for the new repo with Lua\n",
    "}\n",
    "\n",
    "# Adjust your model to include predictions for the new data\n",
    "with language_usage_model:\n",
    "    # Generate predictions\n",
    "    pp_samples = pm.sample_posterior_predictive(\n",
    "        trace,\n",
    "        var_names=['language_count_obs'],\n",
    "        data=new_repo_data\n",
    "    )\n",
    "\n",
    "# Analyze the predictive samples for the new repository\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
