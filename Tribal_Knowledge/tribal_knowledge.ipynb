{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0d3bdff101d217",
   "metadata": {},
   "source": [
    "## Hierarchical Bayesian Modeling to assess tribal knowledge\n",
    "\n",
    "In this analysis, we will try to create a methodology and data-driven metric for identifying potential technological risks within an organization's coding protocols. We'll examine how programming languages are utilised across various projects and repositories, similar to those found in code repositories such as GitHub,  and leverage Hierarchical Bayesian Modelling (HBM) for multi-level data analysis. \n",
    "\n",
    "HBM effectively captures project-specific variances and overall project trends, providing a nuanced \"risk\" metric for Enterprise Architecture. This enables organisations to identify potential knowledge silos and make strategic decisions to enhance project continuity and organisational adaptability.\n",
    "\n",
    "By analysing language usage across different organisational levels and integrating uncertainty, HBM aims to expose pockets of siloed tribal knowledge (in this example, via a proxy of languages used, but can easily be extended to accommodate other features such a #of commits, time since last commit, total commits, etc., etc.), which is crucial for identifying hidden risks within the architectural framework. This analysis uncovers potential vulnerabilities and compares language usage at repository and project scales against wider organisational patterns. These comparative insights are critical, revealing when a technology may seem insignificant in isolation emerges as a considerable risk in the broader organisational context due to limited expertise or exposure. This comprehensive examination ensures that technology decisions are made with a strategic perspective, reinforcing organisational resilience in the face of technological evolution.\n",
    "\n",
    "For a more concrete example, consider a scenario where an organisation's repository primarily uses Haskell, a language not commonly used in broader enterprise contexts. Hierarchical Bayesian Modelling evaluates the risk by scrutinising Haskell's application within the repository, its relevance to the project, and its organisational prevalence. This comprehensive assessment ascertains the alignment of Haskell's use with the enterprise's technological trajectory and knowledge base, guiding strategic architectural decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee99bdbedd31ef3",
   "metadata": {},
   "source": [
    "#### Flow\n",
    "To model this problem space using a Bayesian hierarchical model\n",
    "\n",
    "- **Data Preparation:** Aggregate the language bytes for each language across all repositories and projects.\n",
    "- **Model Definition:** Define a hierarchical model in Pymc using project-level priors influencing repository-level distributions.\n",
    "- **Inference:** Use MCMC provided by pymc to sample from the posterior distribution.\n",
    "- **Analysis:** Analyze the posterior distributions to identify languages with usage outside the credible regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64929205d7d6c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "# Import required packages\n",
    "\n",
    "import polars as pl\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pprint\n",
    "\n",
    "from utils import load_data, json2polars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b0862-2954-49f3-a206-e69b84cddea7",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "The first step is to run the `generate_dummy_data.py` file to make sure we have data to play around with, the generated dummy data is similar to what you might pull from GitHub's REST API for repository languages https://docs.github.com/en/rest/repos/repos?apiVersion=2022-11-28#list-repository-languages\n",
    "\n",
    "```GitHub CLI api\n",
    "https://cli.github.com/manual/gh_api\n",
    "\n",
    "gh api \\\n",
    "  -H \"Accept: application/vnd.github+json\" \\\n",
    "  -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n",
    "  /repos/OWNER/REPO/languages\n",
    "\n",
    "Example Response:\n",
    "{\n",
    "  \"C\": 78769,\n",
    "  \"Python\": 7769\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "From here, we can load and transform the data. In this instance, we will be using `polars` rather than `pandas` dataframes \n",
    "\n",
    "As we are using polars and not pandas, we want to avoid Pandas-style coding and use the Polars Expressions API. \n",
    "Expressions are the heart of Polars and yield the best performance.\n",
    "\n",
    "*N.B.* Here's a section of the User Guide that may help transitioning from Pandas-style coding to using Polars Expressions.\n",
    "https://docs.pola.rs/user-guide/migration/pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bc6912-e1b0-4ff3-bd41-da2c962a92af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dummy_language_data.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df_json \u001B[38;5;241m=\u001B[39m \u001B[43mload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/dummy_language_data.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Prety print some Projects and Repos randomly to visualise the data\u001B[39;00m\n\u001B[1;32m      3\u001B[0m NUM_PROJECTS \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/gitpackages/EA-metrics/Tribal_Knowledge/utils.py:13\u001B[0m, in \u001B[0;36mload_data\u001B[0;34m(filepath)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_data\u001B[39m(filepath):\n\u001B[1;32m      5\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load and parse JSON data from a given filepath.\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03m        dict: The parsed JSON data.\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m     14\u001B[0m         data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/dummy_language_data.json'"
     ]
    }
   ],
   "source": [
    "df_json = load_data(\"data/dummy_language_data.json\")\n",
    "# Prety print some Projects and Repos randomly to visualise the data\n",
    "NUM_PROJECTS = 1\n",
    "first_N_projects = {k: df_json[k] for k in list(df_json)[:NUM_PROJECTS]}\n",
    "pp = pprint.PrettyPrinter(depth=3)\n",
    "pp.pprint(first_N_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479e6ad-ad19-4f50-bc99-495b84ea55e2",
   "metadata": {},
   "source": [
    "Let's flip this into a normal dataset we are used to, and and a new variable to log transform the byte count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a57fbc-6bc4-4c72-bb1f-858e38ba2b57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'with_column'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Loop over the columns to encode\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m column_name \u001B[38;5;129;01min\u001B[39;00m columns_to_encode:\n\u001B[1;32m----> 9\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwith_column\u001B[49m(pl\u001B[38;5;241m.\u001B[39mcol(column_name)\u001B[38;5;241m.\u001B[39mcast(pl\u001B[38;5;241m.\u001B[39mCategorical)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcolumn_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_codes\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Add all new columns to the DataFrame at once\u001B[39;00m\n\u001B[0;32m     12\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwith_columns(new_columns)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'with_column'"
     ]
    }
   ],
   "source": [
    "df=json2polars(df_json)\n",
    "df = df.with_columns(pl.col('ByteCount').log().alias('logByteCount'))\n",
    "# Encode cols to categories rather than str\n",
    "\n",
    "columns_to_encode = ['Project', 'Repository', 'Language']\n",
    "new_columns = []\n",
    "# Loop over the columns to encode\n",
    "for column_name in columns_to_encode:\n",
    "    df = df.with_column(pl.col(column_name).cast(pl.Categorical).alias(f'{column_name}_codes'))\n",
    "\n",
    "# Add all new columns to the DataFrame at once\n",
    "df = df.with_columns(new_columns)\n",
    "\n",
    "\n",
    "# Number of categories for each variable\n",
    "n_projects = df['Project'].unique().count()\n",
    "n_repos = df['Repository'].unique().count() \n",
    "n_languages = df['Language'].unique().count()\n",
    "\n",
    "\n",
    "print(df.head(n=20))\n",
    "print(\"Total number of projects:\", df['Project'].unique().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74fe316-0f6a-4b26-8623-8f30ecb6b171",
   "metadata": {},
   "source": [
    "## Hierarchical Model Specification\n",
    "\n",
    "This section introduces Hierarchical Bayesian Modelling (HBM) principles and their application in structuring complex, multi-level datasets, such as those encountered in evaluating technological risks within coding languages.\n",
    "\n",
    "#### Introduction to Hierarchical Bayesian Modelling\n",
    "Hierarchical Bayesian Modelling is a statistical framework that enables data analysis across different levels of hierarchy by integrating the variability within individual units (such as repositories) and the commonalities across groups (such as projects, domains, or departments). Bayes' theorem is at the core of Bayesian inference, which updates the probability for a hypothesis as more evidence becomes available. One of the critical concepts in HBM is exchangeability, which implies that data points are probabilistically symmetrical. This makes it suitable for modelling data that doesn't have a natural ordering or grouping but is considered identically distributed given some unknown parameters.\n",
    "\n",
    "### Defining the Hierarchical Model Structure\n",
    "\n",
    "When we analyse the usage of programming languages, we're looking at a hierarchical model structure with multiple layers. These layers represent language usage within repositories, which are nested within projects. \n",
    "\n",
    "- **Level 1 - Repository-Level Likelihood:** At this level, we describe the observed data, such as the amount of code written in each language within a repository, using a likelihood function.\n",
    "  \n",
    "  $$ P(Language_{ij} | \\theta_{ij}) \\sim SomeDistribution(\\theta_{ij}) $$\n",
    "\n",
    "  Here, \\($ \\theta_{ij} $\\) is a parameter representing the language usage, where \\($ i $\\) denotes the repository and \\($ j $\\) the language.\n",
    "\n",
    "- **Level 2 - Project-Level Priors:** As we move up to the project level, parameters from the repository level are considered uncertain and are described by priors.\n",
    "\n",
    "  $$ \\theta_{ij} | \\mu_i, \\kappa_i \\sim Beta(\\mu_i \\kappa_i, (1 - \\mu_i) \\kappa_i) $$\n",
    "\n",
    "  The Beta distribution parameters \\($ \\mu_i $\\) and \\($ \\kappa_i $\\) represent the expected language usage and variability within a project.\n",
    "\n",
    "- **Level 3 - Organisational-Level Hyperpriors:** At the organisational level, we look at the broader patterns in language usage across all projects. \n",
    "\n",
    "  $$ \\mu_i \\sim Beta(a_{\\mu}, b_{\\mu}) $$\n",
    "  $$ \\kappa_i \\sim Gamma(a_{\\kappa}, b_{\\kappa}) $$\n",
    "\n",
    "  Hyperpriors for \\($ \\mu_i $\\) and \\($ \\kappa_i $\\) reflect our assumptions about these patterns before analysing the data.\n",
    "\n",
    "This hierarchical approach allows for a detailed analysis of organisational language usage. We're not just modelling individual repositories but also capturing trends across projects and the entire organisation.\n",
    "\n",
    "### Explanation of Model Parameters and Priors\n",
    "\n",
    "In Hierarchical Bayesian Modelling (HBM), parameters are not fixed values but are expressed as distributions, known as priors. Priors encapsulate our initial understanding or beliefs about the parameters before we examine the data. For instance, the use of a programming language within a repository can be characterised by a Beta distribution, representing the proportion of code written in that language.\n",
    "\n",
    "The hyperparameters, denoted as \\($ \\mu_i $\\) and \\($ \\kappa_i $\\) introduce a layer of variability that accounts for differences within repositories and across projects. They are informed by higher-level distributions or hyperpriors, like the Beta distribution for \\($ \\mu_i $\\), which describes the mean language usage, and the Gamma distribution for \\($ \\kappa_i $\\), which relates to the variability of that usage.\n",
    "\n",
    "By employing a hierarchical model, we can refine our initial beliefs in light of the data collected, which leads to a more nuanced understanding of an organisation's coding practices. The model's flexibility allows us to adapt to new information, enhancing the precision of our insights.\n",
    "\n",
    "\n",
    "## Transitioning to Posterior Distributions\n",
    "\n",
    "### From Theory to Practice: The Role of Posterior Distributions\n",
    "\n",
    "With our model parameters defined and their priors set, the next step in Bayesian analysis is to update these beliefs with observed data. This is where the posterior distribution comes into play.\n",
    "\n",
    "\n",
    "#### What is the posterior?\n",
    "\n",
    "In the hierarchical Bayesian modelling (HBM) context, the posterior distribution is the updated belief about our model's parameters after considering the observed data. It combines our prior beliefs (the priors) and the evidence from the data (the likelihood). Mathematically, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(\\theta | data) \\propto P(data | \\theta) \\times P(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($ P(\\theta | data) $\\) is the posterior distribution of the parameters \\($ \\theta $\\).\n",
    "- \\($ P(data | \\theta) $\\) is the likelihood of the data given the parameters.\n",
    "- \\($ P(\\theta) $\\) is the prior distribution of the parameters.\n",
    "\n",
    "After observing the data, the posterior distribution provides a range of likely values for the parameters, which is crucial for making informed decisions.\n",
    "\n",
    "\n",
    "## Practical Implications of Posterior Analysis in Hierarchical Bayesian Modelling\n",
    "\n",
    "Through the lens of Hierarchical Bayesian Modelling, the posterior distribution becomes a beacon, illuminating the path to understanding and action within an organisation's coding practices.\n",
    "\n",
    "\n",
    "### Informing Strategic Decision-Making\n",
    "\n",
    "The power of posterior analysis extends beyond diagnostics; it informs strategic resource allocation and risk management:\n",
    "\n",
    "- **Credible Intervals**: The precision of parameter estimates, reflected in the credible intervals of the posterior distribution, directs our focus to areas where additional data collection or deeper investigation may be warranted.\n",
    "\n",
    "- **Outlier Detection**: Spotting outliers within posterior distributions alerts us to unconventional language usage patterns. These could represent areas of innovation warranting further exploration or potential risks if the languages in question lack broad support.\n",
    "\n",
    "- **Strategic Resource Allocation**: Insights gained from posterior distributions enable informed decisions on resource allocation—be it for targeted training programmes, strategic hiring to build expertise in underutilised languages, or investment in technology stacks that promise to align with and propel the organisation's strategic objectives.\n",
    "\n",
    "Interpreting the posterior distributions derived from our hierarchical model does more than just enhance our understanding of language usage; it equips us to forecast, plan, and foster a coding environment that is both efficient and resilient to future challenges.\n",
    "\n",
    "### Uncovering Knowledge Silos\n",
    "\n",
    "The posterior distributions for language usage within repositories serve as a diagnostic tool, revealing languages that are disproportionately relied upon. Anomalies in these distributions may signal the existence of knowledge silos, suggesting areas where diversification and training could be beneficial. By identifying these silos, we can proactively address potential bottlenecks in knowledge transfer and code maintenance.\n",
    "\n",
    "### Assessing Project-Level Variability\n",
    "\n",
    "The consistency of coding practices across repositories within projects is characterised by project-level hyperparameters. When significant variability is observed in these posterior distributions, it may reflect fragmented coding practices that could undermine team collaboration and project efficiency. This insight drives us to review and possibly revise coding standards, ensuring that practices are aligned and conducive to project success.\n",
    "\n",
    "### Evaluating Organisational Coding Norms\n",
    "\n",
    "At the highest organisational level, posterior distributions offer a macro perspective of coding culture and norms. Deviations in these distributions can reveal organisational preferences or aversions towards specific languages. Understanding these trends is critical for shaping future strategies in technology adoption, capability development, and training initiatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6280421-54eb-4c68-8cb1-f28ebb96f6cb",
   "metadata": {},
   "source": [
    "## Model Implementation\n",
    "- Implementing the HBM using PyMC3\n",
    "- Defining the model in PyMC3\n",
    "- Setting up the priors for each level of the hierarchy\n",
    "- Incorporating the data into the model\n",
    "- Model fitting (e.g., using MCMC methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549433a2-7215-43e3-90f7-f44294519f4f",
   "metadata": {},
   "source": [
    "## Model Diagnostics\n",
    " - Checking model convergence (e.g., trace plots, R-hat statistics)\n",
    " - Posterior predictive checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d0d40-f642-4632-875b-4a9395e2491c",
   "metadata": {},
   "source": [
    "##  Results and Interpretation\n",
    "- Extracting and summarizing the posterior distributions of model parameters\n",
    "- Identifying significant factors and their impacts on language usage risk metrics\n",
    "- Ridge plots for visualizing the distribution of language usage across projects and repositories, highlighting potential outliers or risks\n",
    "- Additional plots for deeper insights (e.g., comparison of language usage trends across different organizational levels)\n",
    "\n",
    "#### Key Considerations:\n",
    "\n",
    "- **Credible Intervals and Outliers**: Narrow credible intervals suggest high parameter estimate certainty, while wide intervals indicate areas needing further investigation. Outlier detection can pinpoint innovative areas or non-standard practices for strategic exploration.\n",
    "\n",
    "- **Decision-Making**: Insights from the HBM should inform strategic decisions regarding technology adoption, project management, and training to align coding practices with organizational goals, enhancing project continuity and adaptability.\n",
    "\n",
    "This combined understanding of HBM principles, model structure, and practical implications equips readers with the knowledge to interpret complex data analyses meaningfully, driving informed decision-making within the organisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a884ea9-56cf-4070-876a-7f4ae955f318",
   "metadata": {},
   "source": [
    "## Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df155dec-0023-41fd-933f-03df830e7b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
